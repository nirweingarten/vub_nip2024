#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
%\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

\PassOptionsToPackage{verbose=true,letterpaper}{geometry}
\PassOptionsToPackage{utf8}{inputenc}

% ready for submission
\usepackage{neurips_2024}

\usepackage{natbib}
\AtBeginDocument{%
  \@ifpackageloaded{natbib}%
    {%
      % When natbib is in use, set the proper style and fix a few things
      \let\cite\citep
      \let\shortcite\citeyearpar
      \setcitestyle{aysep={}}
      \setlength\bibhang{0pt}
      \bibliographystyle{aaai24}
    }{}%
}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%\title{Formatting Instructions For NeurIPS 2024}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{Nir Weingarten, Moshe Butman \& Zohar Yakhini \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Reichman University\\
Herzliya, Israel \\
\texttt{\{nir.weingarten,zohar.yakhini,moshe.butman\}@runi.ac.il} \\
\And
Ran Gilad-Bachrach \\
Department of Computational Biology \\
Tel-Aviv University \\
Tel-Aviv, Israel \\
\texttt{rgb@tau.ac.il} \\
\And
Ronit Bustin \\
Advanced Technical Center \\
General Motors Research \\
Herzliya, Israel \\
\texttt{ronit.bustin@gmail.com} \\}


% roman numerals
% \renewcommand{\theequation}{\roman{equation}}

%\bibliographystyle{../iclr_vub_paper/iclr2024_conference}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\rightmargin 5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A Tighter Bound on the Information Bottleneck with Applications to Deep
 Learning
\end_layout

\begin_layout Abstract
The Information Bottleneck (IB) provides a hypothetically optimal framework
 for data modeling, yet is often intractable.
 Recent efforts optimized supervised DNNs with a variational upper bound
 to the IB objective, resulting in improved robustness to adversarial attacks.
 However, when deriving the upper bound, the supervisor distribution 
\begin_inset Formula $p^{*}(y)$
\end_inset

 is assumed to be constant, where in practice it is optimized over.
 This work demonstrates that lifting this assumption not only results in
 a tighter bound on the IB and improved empirical performance, but also
 proposes a new motivation for conditional entropy regularization.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Deep Neural Nets (DNNs) learn latent representations induced by their downstream
 task, objective function, and other parameters.
 The quality of the learned representations impacts the DNN's generalization
 ability and the coherence of the emerging latent space 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Bengio2009}
\end_layout

\end_inset

.
 A question emerges regarding the extraction of an optimal latent representation
 for all data points, from a restricted set of training examples.
 Classic information theory provides rate-distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Shannon1959}
\end_layout

\end_inset

 for optimal compression of data.
 However, rate-distortion regards all information as equal, not taking into
 account which information is more relevant to a specified downstream task,
 without constructing tailored distortion functions.
 The Information Bottleneck (IB) 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 resolves this limitation by defining mutual information (MI) between the
 learned representation and a designated downstream task as a universal
 distortion function.
 Yet, learning representations using the IB method is possible given discrete
 distributions, and some continuous ones, but not in the general case 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Chechik2003}
\end_layout

\end_inset

.
 Moreover, MI is either difficult or impossible to optimize over when considerin
g deterministic models, such as MLPs 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Saxe2018,Amjad2020}
\end_layout

\end_inset

.
 Nonetheless, the promise of the IB remains alluring, and recents works
 utilized VAE 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 inspired variational approximations to approximate upper bounds to the
 IB objective, allowing its utilization as a loss function for DNNs, where
 the underlying distributions are both continuous and unknown 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Fischer2020,Cheng2020}
\end_layout

\end_inset

.
 These approaches learn representations in supervised settings, without
 knowledge of the underlying distribution 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

, utilizing the learned variational conditional 
\begin_inset Formula $p(y|x)$
\end_inset

 to approximate MI.
 In contrast, non variational IB methods learn representations in unsupervised
 settings, where the stochastic process underlying the observed data is
 known 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999,Chechik2003,Painsky2017}
\end_layout

\end_inset

.
 Nonetheless, when deriving the variational IB objectives, previous research
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Fischer2020,Cheng2020}
\end_layout

\end_inset

 relax the problem by considering the learned representation as the only
 optimized parameter, when in practice the classifier is also optimized.
 We derive a new upper bound for the IB objective, and a subsequent variational
 approximation, by removing this relaxation.
 We show that our bound is tighter than previous bounds, and that our proposed
 loss function is a tighter variational approximation, when considering
 
\begin_inset Formula $p(y|x)$
\end_inset

 as part of the optimization.
 We believe our new derivation is a better adaptation of the IB for supervised
 tasks, and show empirical evidence of improved performance across several
 challenging tasks over different modalities.
 We utilize previous studies on variational representation learning and
 regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018,Pereyra2017,Achille2018}
\end_layout

\end_inset

 to interpret our findings, and conclude that our proposed derivation applies
 regularization over the variational classifier, preventing it from overfitting
 the learned representations, thus enabling greater MI between learned represent
ation and target 
\begin_inset Formula $Y$
\end_inset

.
 The reader is encouraged to refer to the preliminaries provided in Appendix
\begin_inset space ~
\end_inset

A before proceeding.
\end_layout

\begin_layout Section
Related work
\begin_inset CommandInset label
LatexCommand label
name "sec:related_work"

\end_inset


\end_layout

\begin_layout Subsection
Deterministic Information Bottleneck
\begin_inset CommandInset label
LatexCommand label
name "sec:deterministic_ib"

\end_inset


\end_layout

\begin_layout Standard
Classic information theory offers rate-distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Shannon1959}
\end_layout

\end_inset

 to mitigate signal loss during compression: A source 
\begin_inset Formula $X$
\end_inset

 is compressed to an encoding 
\begin_inset Formula $Z$
\end_inset

, such that maximal compression is achieved while keeping the encoding quality
 above a certain threshold.
 Encoding quality is measured by a task specific distortion function: 
\begin_inset Formula $d:X\times Z\mapsto\mathbb{R}^{+}$
\end_inset

.
 Rate-distortion suggests a mapping that minimizes the rate of bits to source
 sample, measured by 
\begin_inset Formula $I(X;Z)$
\end_inset

, that adheres to a chosen allowed expected distortion 
\begin_inset Formula $D\ge0$
\end_inset

.
 The Information Bottleneck (IB) 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 extends rate-distortion by replacing the tailored distortion functions
 with MI over a target distribution: Let 
\begin_inset Formula $Y$
\end_inset

 be the target signal for some specific downstream task, such that the joint
 distribution 
\begin_inset Formula $P^{*}(x,y)$
\end_inset

 is known, and define the distortion function as MI between 
\begin_inset Formula $Z$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
 The IB is the solution to the optimization problem 
\begin_inset Formula $Z:\underset{P(z|x)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;Y)\ge D$
\end_inset

, that can be optimized by minimizing the IB objective 
\begin_inset Formula $\mathcal{L}_{IB}=I(X;Z)-\beta I(Z;Y)$
\end_inset

 over 
\begin_inset Formula $P(z|x)$
\end_inset

.
 The solution to this objective is a function of the Lagrange multiplier
 
\begin_inset Formula $\beta$
\end_inset

, and is a theoretical limit for representation quality, given mutual informatio
n as an accepted metric, as elaborated in more detail in Appendix
\begin_inset space ~
\end_inset

B.
 The IB is in fact an unsupervised soft clustering problem, where each data
 point 
\begin_inset Formula $x$
\end_inset

 is assigned a probability to belong to the different clusters 
\begin_inset Formula $z$
\end_inset

, given the joint distribution of the input and target tasks 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Slonim2002}
\end_layout

\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Chechik2003}
\end_layout

\end_inset

 showed that computing the IB for continuous distributions is hard in the
 general case, and provided a method to optimize the IB objective in the
 case where 
\begin_inset Formula $X,Y$
\end_inset

 are jointly Gaussian and known.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Painsky2017}
\end_layout

\end_inset

 offered a limited linear approximation of the IB for any distribution by
 extracting the jointly Gaussian element of given distributions.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Saxe2018}
\end_layout

\end_inset

 considered the application of the IB objective as a loss function for DNNs,
 and concluded that computing mutual information in deterministic DNNs is
 problematic as the entropy term 
\begin_inset Formula $H(Z|X)$
\end_inset

 for a continuous 
\begin_inset Formula $Z$
\end_inset

 is infinite.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Amjad2020}
\end_layout

\end_inset

 extended this observation and pointed out that for a discrete 
\begin_inset Formula $Z$
\end_inset

 MI becomes a piecewise constant function of its parameters, making gradient
 descent limiting and difficult.
\end_layout

\begin_layout Subsection
Variational Information Bottleneck
\begin_inset CommandInset label
LatexCommand label
name "subsec:variational_approximations"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 introduced the Variational Information Bottleneck (VIB) - a variational
 approximation for an upper bound to the IB objective for DNN optimization.
 Bounds for 
\begin_inset Formula $I(X,Z)$
\end_inset

 and 
\begin_inset Formula $I(Z,Y)$
\end_inset

 are derived from the non negativity of KL divergence, and are used to form
 an upper bound for the IB objective.
 A variational upper bound is derived by replacing intractable distributions
 with variational approximations.
 Using the 'reparameterization trick' 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 a discrete empirical estimation of the variational upper bound is then
 used as a loss function for classifier DNN optimization.
 The subsequent loss function is equivalent to the 
\begin_inset Formula $\beta$
\end_inset

-autoencoder loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Higgins2017}
\end_layout

\end_inset

.
 VIB was evaluated over image classification tasks, and displayed substantial
 improvements in robustness to adversarial attacks, while causing a slight
 reduction in test set accuracy, when compared to equivalent deterministic
 models.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Achille2018}
\end_layout

\end_inset

 extended VIB with a total correlation term, designed to increase latent
 disentanglement.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Fischer2020}
\end_layout

\end_inset

 proposed an IB based loss function named Conditional Entropy Bottleneck
 (CEB), in which the conditional mutual information of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

 given 
\begin_inset Formula $Y$
\end_inset

 is minimized, instead of the unconditional mutual information.
 The CEB loss, 
\begin_inset Formula $L_{CEB}=\underset{Z}{min}I(X;Z|Y)-\gamma I(Y;Z)$
\end_inset

, is designed to minimize all information in 
\begin_inset Formula $Z$
\end_inset

 that is not relevant to the downstream task 
\begin_inset Formula $Y$
\end_inset

, by conditioning over 
\begin_inset Formula $Y$
\end_inset

.
 CEB is equivalent to IB for 
\begin_inset Formula $\gamma=\beta-1$
\end_inset

 following the chain rule of mutual information 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Cover1999elements}
\end_layout

\end_inset

 and the IB Markov chain, as established in Appendix
\begin_inset space ~
\end_inset

B.
 Similarly to VIB, a variational approximation for CEB was proposed as 
\begin_inset Formula $L_{VCEB}=\mathbb{E}_{x,y}log\left(e(z|x)\right)-\mathbb{E}_{x,y}log\left(b(z|y)\right)-\gamma\mathbb{E}_{x,y}log\left(c(y|z)\right)$
\end_inset

 and tested over the FMNIST 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Xiao2017fashionmnist}
\end_layout

\end_inset

 and CIFAR10 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Krizhevsky2009learning}
\end_layout

\end_inset

 datasets.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Fischer2020}
\end_layout

\end_inset

 showed that VIB is a special case of VCEB, where rate is approximated by
 the variational expression 
\begin_inset Formula $\mathbb{E}_{x,y}log\left(e(z|x)\right)-\mathbb{E}_{x,y}log\left(b(z|y)\right)$
\end_inset

 instead of 
\begin_inset Formula $\mathbb{E}_{x,y}log\left(e(z|x)\right)-\mathbb{E}_{x,y}log\left(r(z)\right)$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Geiger2020}
\end_layout

\end_inset

 investigated wether VCEB is a tighter variational approximation to IB than
 VIB, and concluded that no ordering can be established in the general case,
 noting that any empirical improvement VCEB exhibits over VIB is not due
 to a tighter variational bound on the IB, but rather of VCEB being more
 amenable to optimization, or simply a successful loss function in its own
 regard.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Cheng2020}
\end_layout

\end_inset

 proposed CLUB, an upper bound based MI estimator that empirically outperformed
 the popular MINE estimator 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Belghazi2018}
\end_layout

\end_inset

.
 Since CLUB is an upper bound for MI, it was evaluated as a replacement
 to the upper bound for the IB rate term, 
\begin_inset Formula $I(X;Z)$
\end_inset

, proposed in VIB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017}
\end_layout

\end_inset

.
 CLUB based VIB was tested over the MNIST dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Deng2012mnist}
\end_layout

\end_inset

, resulting in a slight improvement in accuracy compared to VIB, without
 reporting adversarial robustness.
 We note that CLUB does not establish a tighter bound on the VIB rate term,
 and subsequently on the IB objective.
 We also note, that our current work derives a tighter bound on IB through
 the IB distortion term, 
\begin_inset Formula $I(Z;Y)$
\end_inset

, and that combining our suggested method with a CLUB bound on rate is an
 interesting avenue for future work.
\end_layout

\begin_layout Subsection
Information theoretic regularization
\begin_inset CommandInset label
LatexCommand label
name "sec:it_regularization"

\end_inset


\end_layout

\begin_layout Standard
Label smoothing 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 and entropy regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

 both regularize classifier DNNs by increasing the entropy of their output.
 This is achieved by either inserting a scaled conditional entropy term
 to the loss function, 
\begin_inset Formula $-\gamma\cdot H\left(p_{\theta}(y|x)\right)$
\end_inset

, or by smoothing the training data labels.
 Applying either of these methods improved test accuracy and model calibration
 on various challenging classification tasks.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 extended the information plane 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 to VAE 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 settings, measuring distortion as MI between input and reconstructed images,
 and rate as KL divergence between variational representation and marginal.
 The limits of representation quality in VAEs are looser than the theoretical
 IB limits, and heavily depend on the chosen variational families of the
 marginal and decoder distributions.
 The closer the families are to the true distributions, the tighter the
 gap to the optimal IB limit.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 showed that VAEs are susceptible to learn low quality representations,
 as the KL regularization term in the ELBO loss might induce very uninformative
 representations, provided there's a strong enough decoder to compensate
 for bad embeddings by overfitting them.
 This work is further elaborated on in Appendix
\begin_inset space ~
\end_inset

B.
 In the current study, a conditional entropy term 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

 emerges from our proposed derivation of a variational IB loss function,
 and we extend the theoretic framework proposed in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

 to interpret why this new term facilitates a better variational approximation
 of the IB objective.
\end_layout

\begin_layout Section
From VIB to VUB
\begin_inset CommandInset label
LatexCommand label
name "sec:vub"

\end_inset


\end_layout

\begin_layout Standard
As elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:deterministic_ib}
\end_layout

\end_inset

, the IB objective, 
\begin_inset Formula $\mathcal{L}_{IB}=I(X;Z)-\beta I(Z;Y)$
\end_inset

, is computed over the joint distribution 
\begin_inset Formula $p^{*}(x,y,z)$
\end_inset

.
 When 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

 is given, this expression is optimized over the distribution 
\begin_inset Formula $p(z|x)$
\end_inset

, as proposed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Tishby1999}
\end_layout

\end_inset

: 
\begin_inset Formula $Z:\underset{p(z|x)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;Y)\ge D$
\end_inset

.
 However, adapting IB to supervised tasks admits the learned classifier
 as a new RV to the optimization problem.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Geiger2020}
\end_layout

\end_inset

 suggested the Markov chain 
\begin_inset Formula $Y\leftrightarrow X\leftrightarrow Z\leftrightarrow\tilde{Y}$
\end_inset

 for supervised IB, distinguishing between the true unknown RV 
\begin_inset Formula $Y$
\end_inset

, and the learned classifier 
\begin_inset Formula $\tilde{Y}$
\end_inset

.
 Following this logic, we argue that supervised IB optimization should be
 defined as 
\begin_inset Formula $Z,\tilde{Y}:\underset{p(z|x),p(\tilde{y}|z)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;\tilde{Y})\ge D$
\end_inset

.
 The VIB loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017}
\end_layout

\end_inset

 consists of a cross entropy (
\begin_inset Formula $CE$
\end_inset

) term, and a beta modulated KL regularization term, as in 
\begin_inset Formula $\beta$
\end_inset

-VAE loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Higgins2017}
\end_layout

\end_inset

.
 The KL term is derived from a bound on the IB rate term 
\begin_inset Formula $I(X;Z)$
\end_inset

, while the 
\begin_inset Formula $CE$
\end_inset

 term is derived from a bound on the IB distortion term 
\begin_inset Formula $I(Z;Y)=H(Y)-H(Y|Z)$
\end_inset

.
 During the derivation of the CE term, a relaxation is performed such that
 the term 
\begin_inset Formula $H(Y)$
\end_inset

 is assumed constant, and hence ignored, while the term 
\begin_inset Formula $-H(Y|Z)$
\end_inset

 is derived as CE between true and learned supervisor distributions 
\begin_inset Formula $p^{*}(y),p(\tilde{y})$
\end_inset

.
 We derive a new upper bound for the IB objective by not omitting 
\begin_inset Formula $H(Y)$
\end_inset

 from the distortion term.
 Subsequently, the variational approximation for our proposed bound is tighter,
 when taking into account that the optimization process is done over 
\begin_inset Formula $p(\tilde{y}|z)$
\end_inset

 as well as 
\begin_inset Formula $p(z|x)$
\end_inset

.
 This modification attains a tighter variational bound on the IB objective
 for any 
\begin_inset Formula $Y$
\end_inset

 with positive entropy, and a tighter empirical bound for all 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Subsection
IB upper bound
\begin_inset CommandInset label
LatexCommand label
name "sec:ib_upper_bound"

\end_inset


\end_layout

\begin_layout Standard
We begin by establishing a new upper bound for the IB objective by bounding
 the mutual information terms, using the same method as in VIB.
 
\end_layout

\begin_layout Standard

\bar under
Consider 
\begin_inset Formula $I(Z;X)$
\end_inset


\bar default
:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;X)=\int\int p^{*}(x,z)log\left(p^{*}(z|x)\right)dxdz- & \int p^{*}(z)log\left(p^{*}(z)\right)dz\label{eq:i_z_x1}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $r$
\end_inset

 we have that 
\begin_inset Formula $D_{KL}\left(p^{*}(z)\big|\big|r(z)\right)\ge0$
\end_inset

, it follows that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\int p^{*}(z)log\left(p^{*}(z)\right)dz\ge\int p^{*}(z)log\left(r(z)\right)dz\label{eq:d_kl}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And so, by Equation
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:d_kl}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
I(Z;X)\le\int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\label{eq:i_z_x2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\bar under
Consider 
\begin_inset Formula $I(Z;Y)$
\end_inset


\bar default
:
\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $c$
\end_inset

 we have that 
\begin_inset Formula $D_{KL}\left(p^{*}(y|z)\big|\big|c(y|z)\right)\ge0$
\end_inset

, it follows that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\int p^{*}(y|z)log\left(p^{*}(y|z)\right)dy\ge\int p^{*}(y|z)log\left(c(y|z)\right)dy\label{eq:d_kl2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And so, by Equation
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:d_kl2}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)= & \int\int p^{*}(y,z)log\left(\frac{p^{*}(y,z)}{p^{*}(y)p^{*}(z)}\right)dydz\ge\int\int p^{*}(y|z)p^{*}(z)log\left(\frac{c(y|z)}{p^{*}(y)}\right)dydz\nonumber \\
= & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz+H_{p^{*}}(Y)\label{eq:i_z_y}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We now diverge from the original VIB derivation by replacing 
\begin_inset Formula $H_{p^{*}}(Y)$
\end_inset

 with 
\begin_inset Formula $H_{c}(Y|Z)$
\end_inset

 instead of omitting it.
 In addition, we limit the new term to make sure that the inequality holds:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)\ge\int\int p^{*}(y,z)log\left(c(y|z)\right)dydz+min\left\{ H_{p^{*}}(Y),H_{c}(Y|Z)\right\} \label{eq:i_z_y2}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We develop the first term in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_y2}
\end_layout

\end_inset

 using the IB Markov chain 
\begin_inset Formula $Z\leftrightarrow X\leftrightarrow Y$
\end_inset

 and total probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)\ge & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz\nonumber \\
+ & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \label{eq:i_z_y3}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Denote 
\begin_inset Formula $\tilde{Y}$
\end_inset

 as a RV over the same support as 
\begin_inset Formula $Y$
\end_inset

, such that 
\begin_inset Formula $Y\leftrightarrow X\leftrightarrow Z\leftrightarrow\tilde{Y}$
\end_inset

.
 We join Equation 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_x2}
\end_layout

\end_inset

 with Equation 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_y3}
\end_layout

\end_inset

 to establish a new upper bound for the IB objective:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
L_{IB}\le L_{UB}\equiv\beta & \int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\nonumber \\
- & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c_{\text{\ensuremath{\tilde{y}|z}}}(y|z)\right)dxdydz\nonumber \\
- & min\left\{ H_{p^{*}}(Y),-\int\int c(\tilde{y},z)log\left(c(\tilde{y}|z)\right)d\tilde{y}dz\right\} \label{eq:l_ub}
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Variational approximation
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $e(z|x)$
\end_inset

 a variational encoder approximating 
\begin_inset Formula $p^{*}(z|x)$
\end_inset

, and 
\begin_inset Formula $c(\tilde{y}|z)$
\end_inset

 a variational classifier approximating 
\begin_inset Formula $p^{*}(y|z)$
\end_inset

.
 We define the variational approximation 
\begin_inset Formula $L_{VUB}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
L_{UB}\approx L_{VUB}\equiv & \beta\int\int p^{*}(x)e(z|x)log\left(\frac{e(z|x)}{r(z)}\right)dxdz\nonumber \\
- & \int\int\int p^{*}(x)p^{*}(y|x)e(z|x)log\left(c_{\ensuremath{\tilde{y}|z}}(y|z)\right)dxdydz\label{eq:l_vub}\\
- & min\Bigg\{ H_{p^{*}}(Y),-\int\int\int p^{*}(x)e(z|x)c(\tilde{y}|z)log\left(c(\tilde{y}|z)\right)dxd\tilde{y}dz\Bigg\}\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Empirical estimation
\end_layout

\begin_layout Standard
The true and possibly continuous distribution 
\begin_inset Formula $p^{*}(x,y)=p^{*}(y|x)p^{*}(x)$
\end_inset

 can be estimated by Monte Carlo sampling from a discrete dataset 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathcal{S}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
.
 Distributions featuring 
\begin_inset Formula $Z$
\end_inset

 are sampled from a stochastic encoder: Let 
\begin_inset Formula $e_{\phi}(z|x)\sim N(\mu,\Sigma)$
\end_inset

 be a stochastic DNN encoder with parameters 
\begin_inset Formula $\phi$
\end_inset

, and a final layer of dimension 
\begin_inset Formula $2K$
\end_inset

, such that for each forward pass, the first 
\begin_inset Formula $K$
\end_inset

 entries are used to encode 
\begin_inset Formula $\mu$
\end_inset

, and the last 
\begin_inset Formula $K$
\end_inset

 entries to encode a diagonal 
\begin_inset Formula $\Sigma$
\end_inset

, after a soft-plus transformation.
 For each 
\begin_inset Formula $x_{n}\in\mathcal{S}$
\end_inset

 we generate a sample 
\begin_inset Formula $\hat{z}_{n}$
\end_inset

 from the encoder, using the reparameterization trick 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

.

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 Let 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $C_{\lambda}$
\end_inset

 be a discrete classifier neural net parameterized by 
\begin_inset Formula $\lambda$
\end_inset

, such that 
\begin_inset Formula $C_{\lambda}(\tilde{y}|z)\sim Categorical$
\end_inset

, let 
\begin_inset Formula $\hat{H}_{\mathcal{S}}(Y)$
\end_inset

 be the empirical entropy of the true RV 
\begin_inset Formula $Y$
\end_inset

, as measured from the training dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

, and let 
\begin_inset Formula $\tilde{Y}$
\end_inset

 be the learned classifier.
 We chose a standard Gaussian as a variational approximation for the marginal
 
\begin_inset Formula $r(z)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{L}_{VUB}\equiv\frac{1}{N}\sum_{n=1}^{N}\left[\beta D_{KL}\left(e_{\phi}(z|x_{n})\Bigg|\Bigg|r(z)\right)-log\left(C_{\lambda}\left(y_{n}\big|\hat{z}_{n}\right)\right)-min\left\{ \hat{H}_{\mathcal{S}}(Y),H_{C_{\lambda}}\left(\tilde{Y}|Z\right)\right\} \right]\label{eq:l_vub_empirical}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Intuition
\end_layout

\begin_layout Standard
Based on Equation
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_y}
\end_layout

\end_inset

 and our definition of 
\begin_inset Formula $\tilde{Y}$
\end_inset

 in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:ib_upper_bound}
\end_layout

\end_inset

, we give the following formulation of the Barber-Agakov bound and identity
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Barber2003}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
I(Z;Y)\ge\int\int p^{*}(y,z)log\left(c_{\ensuremath{\tilde{y}|z}}(y|z)\right)dydz+H_{p^{*}}(Y)\equiv\tilde{I}(Z;Y)\label{eq:barber_agakov}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The following inequality holds for all distributions of 
\begin_inset Formula $Y$
\end_inset

 with non negative entropy, assuming 
\begin_inset Formula $H_{p^{*}}(Y)\gtrsim H_{c}(\tilde{Y}|Z)$
\end_inset

 as should follow from a well fitted model:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H_{p^{*}}(Y)\ge I(Z;Y)\ge\tilde{I}(Z;Y)\gtrsim\int\int p^{*}(y,z)log\left(c_{\ensuremath{\tilde{y}|z}}(y|z)\right)dydz+H_{c}(\tilde{Y}|Z)\label{eq:mi_squeeze}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We have that the true MI 
\begin_inset Formula $I(Z;Y)$
\end_inset

 is squeezed by the Barber-Agakov MI 
\begin_inset Formula $\tilde{I}(Z;Y)$
\end_inset

 from below, which is in term squeezed by the VUB distortion term.
 Previous variational IB derivations 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Fischer2020,Cheng2020}
\end_layout

\end_inset

 omit the entropy term 
\begin_inset Formula $H_{p^{*}}(Y)$
\end_inset

, and derive the term 
\begin_inset Formula $\int\int p^{*}(y,z)log\left(c_{\tilde{y}|z}(y|z)\right)dydz$
\end_inset

 into cross entropy, which is minimized during optimization.
 Assuming that cross entropy is indeed minimized, intuition suggests that
 increasing the entropy term 
\begin_inset Formula $H_{c}(\tilde{Y}|Z)$
\end_inset

 as close as possible to 
\begin_inset Formula $H_{p^{*}}(Y)$
\end_inset

 will push the true MI 
\begin_inset Formula $I(Z;Y)$
\end_inset

 closer to its theoretical limit.
 Since the optimization cannot change the entropy of the true RV 
\begin_inset Formula $Y$
\end_inset

, the suggested increase in 
\begin_inset Formula $I(Z;Y)$
\end_inset

 can only be caused by learning a more informative representation 
\begin_inset Formula $Z$
\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{venn_diagram}
\end_layout

\end_inset

 illustrates the possible effects of an increase in variational entropy:
 The left hand diagram suggest a model with low variational entropy, and
 a low variational mutual information, and the right hand diagram suggest
 a model with high variational entropy.
 Reduction in cross entropy increases 
\begin_inset Formula $\tilde{I}(Z;Y)$
\end_inset

, and the true mutual information 
\begin_inset Formula $I(Z;Y)$
\end_inset

 increases as a result of the Barber-Agakov inequality 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Barber2003}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/venn_diagram_new.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Venn diagrams illustrating the possible increase in true mutual information
 as a result of increased variational entropy.
 The right diagram features higher variational entropy, and higher variational
 mutual information, that induce higher true mutual information as a result
 of the Barber-Agakov inequality 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Barber2003}
\end_layout

\end_inset

.We disregard change in the value of 
\begin_inset Formula $Z$
\end_inset

 and the ratio between 
\begin_inset Formula $Z$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 to simplify the figure and focus on the relation between true MI and variationa
l entropy.
\begin_inset CommandInset label
LatexCommand label
name "venn_diagram"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Experiments
\begin_inset CommandInset label
LatexCommand label
name "sec:Experiments"

\end_inset


\end_layout

\begin_layout Standard
We follow the experimental setup proposed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

, extending it to NLP tasks as well.
 We trained image classification models on the ImageNet 2012 dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{deng2009imagenet}
\end_layout

\end_inset

, and text classification on
\series bold
 
\series default
the IMDB
\series bold
 
\series default
sentiment analysis dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{maas2011imdb}
\end_layout

\end_inset

.
 For each dataset, we compared a competitive Vanilla model with a VIB and
 a VUB model trained with beta values of 
\begin_inset Formula $\beta=10^{-i}$
\end_inset

 for 
\begin_inset Formula $i\in\{1,2,3\}$
\end_inset

.
 Each model was trained and evaluated 5 times per 
\begin_inset Formula $\beta$
\end_inset

 value, with consistent performance and statistical significance shown by
 a Wilcoxon rank sum test.
 Each model was evaluated using test set accuracy, and robustness to various
 adversarial attacks.
 For image classification, we employed the untargeted Fast Gradient Sign
 (FGS) attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Goodfellow2015}
\end_layout

\end_inset

 as well as the targeted CW 
\begin_inset Formula $L_{2}$
\end_inset

 attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Carlini2017}, 
\backslash
cite{Kaiwen2018}
\end_layout

\end_inset

.
 For text classification, we used the untargeted Deep Word Bug attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{gao2018deepwordbug}, 
\backslash
cite{Morris2020}
\end_layout

\end_inset

 as well as the untargeted PWWS attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ren2019pwws}
\end_layout

\end_inset

.
 Elaboration on the experimental setup, results and further insights from
 the experiments are available in Appendix
\begin_inset space ~
\end_inset

C.
 Code to reconstruct the experiments is provided to the supplementary materials
 of this paper.
\end_layout

\begin_layout Subsection
Image classification
\end_layout

\begin_layout Standard
A pre-trained inceptionV3 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 base model was used and achieved a 77.21% accuracy on the ImageNet 2012
 validation set (Test set for ImageNet is unavailable).
 Image classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figures
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:untargeted_examples}, 
\backslash
ref{fig:targeted_examples}
\end_layout

\end_inset

 in Appendix
\begin_inset space ~
\end_inset

C.
 The empirical results presented in Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

 confirm that while VIB reduces performance on the validation set, it substantia
lly improves robustness to adversarial attacks.
 Moreover, these results demonstrate that VUB significantly outperforms
 VIB in terms of validation accuracy, while providing competitive robustness
 to attacks similarly to VIB.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="5">
<features booktabs="true" tabularvalignment="middle" tabularwidth="12cm">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Val 
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.1}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.5}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
CW
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
77.2%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
68.9%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
67.7%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
788
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
73.7%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
59.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
63.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3917
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm291$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
72.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
53.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
72.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.01\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
58.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.03\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
62.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
66.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2666
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm140$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.05\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
57.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64.3%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1564
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm218$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
74.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.09\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
57.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3575
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm456$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ImageNet evaluation scores for vanilla, VIB and VUB models, average over
 5 runs with standard deviation.
 First column is performance on the ImageNet validation set (higher is better
 
\begin_inset Formula $\uparrow$
\end_inset

), second and third columns are the % of successful FGS attacks at 
\begin_inset Formula $\epsilon=0.1,0.5$
\end_inset

 (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

), and the fourth column is the average 
\begin_inset Formula $L_{2}$
\end_inset

 distance for a successful Carlini Wagner 
\begin_inset Formula $L_{2}$
\end_inset

 targeted attack (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

).
 VUB attains significantly higher accuracy over unseen data in all settings,
 while preserving competitive robustness to adversarial attacks.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:imagenet_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Text classification
\begin_inset CommandInset label
LatexCommand label
name "sec:text_classification"

\end_inset


\end_layout

\begin_layout Standard
A fine tuned BERT uncased 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Devlin2019}
\end_layout

\end_inset

 base model was used and achieved a 93.0% accuracy on the
\series bold
 
\series default
IMDB sentiment analysis test set.
 Text classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_examples}
\end_layout

\end_inset

 in Appendix
\begin_inset space ~
\end_inset

C.
 In this modality, VUB significantly outperforms VIB in both test set accuracy
 and robustness to the two attacks.
 Moreover, VUB also outperformed the original model in terms of test set
 accuracy.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="4">
<features booktabs="true" tabularvalignment="middle" tabularwidth="12cm">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Test
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
DWB
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
PWWS
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
93.0%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
54.3%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
100%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
91.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
35.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.4\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
41.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm6.6\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
90.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
41.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
62.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm14.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
89.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.9\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
90.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm8.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
99.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.9\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
93.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
27.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
28.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
92.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
30.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
50.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
89.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
99.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
100%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
IMDB evaluation scores for vanilla, VIB and VUB models, average over 5 runs
 with standard deviation.
 First column is performance over the test set (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

), second is % of successful Deep Word Bug attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

) and third is % of successful PWWS attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

).
 In almost all cases VUB attains significantly higher accuracy over unseen
 data, as well as significantly higher robustness to adversarial attacks.
 For this modality, VUB also outperforms the vanilla model in terms of test
 set accuracy for 
\begin_inset Formula $\beta=10^{-3}$
\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
The IB is a private case of rate distortion, and was initially designed
 as to optimized compressed representations.
 Adapting the IB objective for supervised tasks results in optimization
 of a classifier distribution as well, and requires a reformulation of the
 initial problem to include both representation and discriminator.
 Following this logic, assuming a constant 
\begin_inset Formula $H(Y)$
\end_inset

 relaxes the problem, and lifting this assumption lead us to derive a tighter
 variational bound over the optimized objective.
 When used as a loss function, our proposed bound produces significantly
 better classification accuracy, with equivalent or superior robustness
 to adversarial attacks, over high dimensional tasks of different modalities,
 with high statistical significance.
 On a practical level, the conditional entropy term that follows from our
 proposed derivation provides strong classifier regularization, as shown
 in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

.
 This type of regularization is a possible remedy to the imbalances inherit
 in the ELBO loss function, and correlatively to VIB, as described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

.
 In addition, we propose an possible intuitive motivation for conditional
 entropy regularization, by showing that in the extreme cases high variational
 entropy can squeeze the true mutual information 
\begin_inset Formula $I(Z;Y)$
\end_inset

 higher, implying better representations learned.
 While other advancements have been done in recent years, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Fischer2020,Cheng2020}
\end_layout

\end_inset

, they focus on modifications to the IB rate term, and none show a tighter
 bound than VIB.
 In contrast, our work derives a provable upper bound by modifying the distortio
n term.
\end_layout

\begin_layout Standard
Applying variational IB as an objective to supervised learning relies on
 three assumptions: (1) It suffices to optimize the mutual information metric
 to optimize a models performance; (2) Forgetting more information about
 the input, while keeping relevant information about the output, induces
 better generalization; (3) Mutual information between the input, output
 and latent representation can be approximated to a desired level of accuracy.
 Our improved empirical results, induced by a tighter bound, suggest better
 data modeling, and strengthen the cause for variational IB as an objective
 for classifier DNNs.
 A possible counter argument is, that the improvments in adversarial robustness
 of variational IB DNNs is an artifact of their latent geometry, rather
 than the quality of their learned representations.
 As the KL regularization induces a smoother latent spce, minor perturbations
 will not cause a significant change in latent semantics, possibly making
 the models more robust to attacks.
 Nonetheless, VUB is presented as a tractable and tighter upper bound of
 the IB objective, that can be easily adapted to any classifier DNN to significa
ntly increase robustness to various adversarial attacks, while inflicting
 minimal decrease in test set performance, and in some cases even increasing
 it.
 
\end_layout

\begin_layout Standard
This study opens many opportunities for further research: Further improvements
 to the upper bound, including combining VUB with the CLUB bound on rate
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Cheng2020}
\end_layout

\end_inset

; Applying VUB in self-supervised learning, and in particular to measure
 whether representations learned with VUB capture better semantics than
 representations learned with non IB inspired loss functions; Finally, experimen
ts with a full covariance matrix VUB, and studying the effects of latent
 geometry on adversarial robustness is left to future work.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bibliography{draft.bib}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Section*
\start_of_appendix
Appendix A - Preliminaries
\begin_inset CommandInset label
LatexCommand label
name "sec:AppendixA"

\end_inset


\end_layout

\begin_layout Standard
We denote random variables (RVs) with upper cased letters 
\begin_inset Formula $X,Y$
\end_inset

, and their realizations in lower case 
\begin_inset Formula $x,y$
\end_inset

.
 Denote discrete Probability Mass Functions (PMFs) with an upper case 
\begin_inset Formula $P(x)$
\end_inset

 and continuous Probability Density Functions (PDFs) with a lower case 
\begin_inset Formula $p(x)$
\end_inset

.
 Subscripts are written where the RVs identities are not clear from the
 context, and hat notation denotes empirical measurements.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X,Y$
\end_inset

 be two observed random variables with a true and unknown joint distribution
 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

, and true marginals 
\begin_inset Formula $p^{*}(x),\,p^{*}(y)$
\end_inset

.
 We can attempt to approximate these distributions using a model 
\begin_inset Formula $p_{\theta}$
\end_inset

 with parameters 
\begin_inset Formula $\theta$
\end_inset

, such that for generative tasks 
\begin_inset Formula $p_{\theta}(x)\approx p^{*}(x)$
\end_inset

, and for discriminative tasks 
\begin_inset Formula $p_{\theta}(y|x)\approx p^{*}(y|x)$
\end_inset

, using a dataset of 
\begin_inset Formula $N$
\end_inset

 i.i.d observation pairs 
\begin_inset Formula $\mathcal{S}=\left\{ (x_{1},y_{1}),...,(x_{N},y_{N})\right\} $
\end_inset

 to fit our model.
 One can also assume the existence of an additional unobserved RV 
\begin_inset Formula $Z\sim p^{*}(z)$
\end_inset

 that influences or generates the observed RVs 
\begin_inset Formula $X,Y$
\end_inset

.
 Since 
\begin_inset Formula $Z$
\end_inset

 is unobserved, it is absent from the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

, and so cannot be modeled directly.
 Denote 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x|z)p_{\theta}(z)dz=\int p_{\theta}(x,z)dz$
\end_inset

 the marginal, 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

 the prior as it is not conditioned over any other RV, and 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

 the posterior following Bayes' rule.
\end_layout

\begin_layout Standard
When modeling an unobserved variable of an unknown distribution, we encounter
 a problem as the marginal 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x,z)dz$
\end_inset

 doesn't have an analytic solution.
 This intractability can be overcome by choosing some tractable parametric
 variational distribution 
\begin_inset Formula $q_{\phi}(z|x)$
\end_inset

 to approximate the posterior 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

, such that 
\begin_inset Formula $q_{\phi}(z|x)\approx p_{\theta}(z|x)$
\end_inset

, and estimate 
\begin_inset Formula $p_{\theta}(x,z)$
\end_inset

 or 
\begin_inset Formula $p_{\theta}(x,z|y)$
\end_inset

 by fitting the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2019}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Vapnik1995}
\end_layout

\end_inset

 defines 
\emph on
supervised
\emph default
 learning as follows:
\end_layout

\begin_layout Itemize
A generator of random vectors 
\begin_inset Formula $x\in\mathbb{R}^{d}$
\end_inset

, drawn independently from an unknown probability distribution 
\begin_inset Formula $p^{*}(x)$
\end_inset

.
\end_layout

\begin_layout Itemize
A supervisor who returns a scalar output value 
\begin_inset Formula $y\in\mathbb{R}$
\end_inset

, according to an unknown conditional probability distribution 
\begin_inset Formula $p^{*}(y|x)$
\end_inset

.
 We note that these probabilities can indeed be soft labels, where 
\begin_inset Formula $y$
\end_inset

 is a continuous probability vector, rather the more commonly used hard
 labels.
\end_layout

\begin_layout Itemize
A learning machine capable of implementing a predefined set of functions,
 
\begin_inset Formula $f(x,\theta):\mathbb{R}^{d}\times\Theta\mapsto\mathbb{R}$
\end_inset

, where 
\begin_inset Formula $\Theta$
\end_inset

 is a set of parameters.
\end_layout

\begin_layout Standard
The problem of supervised learning is that of choosing from the given set
 of functions, the one that best approximates the supervisors response,
 based on observation pairs from the training set 
\begin_inset Formula $\mathcal{S}$
\end_inset

, drawn according to 
\begin_inset Formula $p^{*}(x,y)=p^{*}(x)p^{*}(y|x)$
\end_inset

.
\end_layout

\begin_layout Standard
Given a set of unlabeled data points 
\begin_inset Formula $\{x_{1},...,x_{N}\},x_{i}\in\mathbb{R}^{d}$
\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Slonim2002}
\end_layout

\end_inset

 defines 
\emph on
unsupervised
\emph default
 learning as the task of constructing a compact representation of these
 points, which in some sense reveals their hidden structure.
 This representation can be used further to achieve a variety of goals,
 including reasoning, prediction, communication etc.
 In particular, unsupervised clustering partitions the data points into
 exhaustive and mutually exclusive clusters, where each cluster can be represent
ed by a centroid, typically a weighted average of the cluster's members.
 
\emph on
Soft clustering
\emph default
 assigns cluster probabilities for each data point, and fits an assignment
 by minimizing the expected loss for these probabilities, usually a distance
 metric such as MSE.
\end_layout

\begin_layout Standard
In this work, information theoretic functions share the same notation for
 discrete and continuous settings, and are denoted as follows:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="4">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="middle" width="2cm">
<column alignment="left" valignment="middle" width="2.2cm">
<column alignment="left" valignment="middle" width="4.1cm">
<column alignment="left" valignment="middle" width="4.1cm">
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Notation
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\series bold
Differential
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\series bold
Discrete
\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $H_{p}(X)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $-\int p(x)log\left(p(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\sum_{x\in X}P(x)log\left(P(x)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Conditional
\end_layout

\begin_layout Plain Layout

\series bold
entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $H_{p}(X|Y)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\int\int p(x,y)log\left(p(x|y)\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $dxdy$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\sum_{x\in X}\sum_{y\in Y}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $P(x,y)log\left(P(x|y)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Cross
\end_layout

\begin_layout Plain Layout

\series bold
entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $CE(p,q)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\int p(x)log\left(q(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\sum_{x\in X}P(x)log\left(Q(x)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Joint
\end_layout

\begin_layout Plain Layout

\series bold
entropy
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $H_{p}(X,Y)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\int\int p(x,y)log\left(p(x,y)\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $dxdy$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\sum_{x\in X}\sum_{y\in Y}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $P(x,y)log\left(P(x,y)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
KL
\end_layout

\begin_layout Plain Layout

\series bold
divergence
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $D_{KL}\left(p\big|\big|q\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\int p(x)log\left(\frac{p(x)}{q(x)}\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sum_{x\in X}P(x)log\left(\frac{P(x)}{Q(x)}\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Mutual
\end_layout

\begin_layout Plain Layout

\series bold
information (MI)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $I(X;Y)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\int\int p(x,y)log\left(\frac{p(x,y)}{p(x)p(y)}\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $dxdy$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sum_{x\in X}\sum_{y\in Y}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $P(x,y)log\left(\frac{P(x,y)}{P(x)P(y)}\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Section*
\start_of_appendix
Appendix B - Related work elaboration
\begin_inset CommandInset label
LatexCommand label
name "sec:AppendixB"

\end_inset


\end_layout

\begin_layout Standard
This appendix supplements the related work presented in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:related_work}
\end_layout

\end_inset

, by providing a deeper review of the IB, the IB theory of deep learning,
 and variational approximations for the IB.
\end_layout

\begin_layout Subsection*
The Information Plane
\end_layout

\begin_layout Standard
As mentioned in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:deterministic_ib}
\end_layout

\end_inset

, the solution to the IB objective, 
\begin_inset Formula $\mathcal{L}_{IB}=I(X;Z)-\beta I(Z;Y)$
\end_inset

, depends on the Lagrange multiplier 
\begin_inset Formula $\beta$
\end_inset

.
 Hence, the IB objective has no one unique solution, and can thus be plotted
 as a function of 
\begin_inset Formula $\beta$
\end_inset

 and of 
\begin_inset Formula $Z$
\end_inset

's cardinality, over a Cartesian system composed of the axes 
\begin_inset Formula $I(X;Z)$
\end_inset

 (rate) and 
\begin_inset Formula $I(Z;Y)$
\end_inset

 (distortion).
 We denote the resulting curve the 
\emph on
information curve
\emph default
, and its Cartesian system the 
\emph on
information plane
\emph default
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

, as illustrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

.
 When 
\begin_inset Formula $\beta$
\end_inset

 approaches 
\begin_inset Formula $0$
\end_inset

 the distortion term is nullified and we learn a representation that has
 no information over the down stream task, and maximal compression (such
 a representation may be a null vector), and when 
\begin_inset Formula $\beta$
\end_inset

 approaches 
\begin_inset Formula $\infty$
\end_inset

 we learn a representation that has the maximal possible information over
 the downstream task.
 The region above the information curve is unreachable by any possible represent
ation.
 The different bifurcation of the information curve, illustrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

, correspond to the different possible cardinalities of the compressed represent
ation.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/info_plane.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The information plane and curve: rate-distortion ratio over 
\begin_inset Formula $\beta$
\end_inset

.
 At 
\begin_inset Formula $\beta=0$
\end_inset

 the representation is compressed but uninformative (maximal compression),
 at 
\begin_inset Formula $\beta\rightarrow\infty$
\end_inset

 the representation is informative but potentially overfitted (maximal informati
on).
 Taken from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Slonim2002}
\end_layout

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Fixing a Broken Elbo
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Kingma2014}
\end_layout

\end_inset

 introduced variational auto encoders (VAEs) as a latent model based generative
 DNN architecture.
 In VAEs, an unobserved RV 
\begin_inset Formula $Z$
\end_inset

 is assumed to generate evidence 
\begin_inset Formula $X$
\end_inset

, a variational DNN encoder 
\begin_inset Formula $e(z|x)$
\end_inset

 is used to approximate the intractable posterior 
\begin_inset Formula $p^{*}(z|x)$
\end_inset

, and a variational DNN decoder 
\begin_inset Formula $d(\hat{x}|z)$
\end_inset

 is used to reconstruct 
\begin_inset Formula $X$
\end_inset

.
 The log probability 
\begin_inset Formula $log\left(p^{*}(x)\right)$
\end_inset

 is developed in to the tractable Evidence Lower Bound (ELBO) loss: 
\begin_inset Formula $log\left(p^{*}(x)\right)\le\mathcal{L}_{\text{ELBO}}(x)\equiv-\mathbb{E}_{e(z|x)}\left[log\left(d(x|z)\right)\right]+D_{KL}\left(e(z|x)\big|\big|m(z)\right)$
\end_inset

, consisting of a reconstruction error term (cross entropy), and a KL regulariza
tion term between encoder and variational marginal 
\begin_inset Formula $m(z)$
\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 adapt the information plane 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 to VAEs by defining an additional theoretical bound for the ratio between
 rate and distortion, imposed by the limits of finite parametric families
 of variational approximations.
 Instead of true rate and distortion, the proposed information plane features
 variational rate as 
\begin_inset Formula $R\equiv D_{KL}\left(e(z|x)\big|\big|m(z)\right)$
\end_inset

, and variational distortion as 
\begin_inset Formula $D\equiv-\int\int p^{*}(x)e(z|x)log\left(d(x|z)\right)dxdz$
\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{phase_diagram}
\end_layout

\end_inset

 illustrates the suggsted information plane, which is divided into three
 sub planes: (1) Infeasible: This is the IB theoretical limit (As per Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

); (2) Feasible: Attainable given an infinite model family, and complete
 variety of 
\begin_inset Formula $e(z|x),d(x|z)$
\end_inset

 and 
\begin_inset Formula $m(z)$
\end_inset

; (3) Realizable: Attainable given a finite parametric and tractable variational
 family.
 The black diagonal line at the lower left satisfies 
\begin_inset Formula $H_{p^{*}}(X)-D=R$
\end_inset

, resulting in tight variational bounds on the mutual information.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 observe that the variational rate 
\begin_inset Formula $R$
\end_inset

 does not depend on the variational decoder distribution 
\begin_inset Formula $d(x|z)$
\end_inset

.
 As 
\begin_inset Formula $R$
\end_inset

 is used as the ELBO KL regularizer, high variational compression rates
 can be attained regardless of MI between decoder and learned representation.
 Equivalently, good reconstruction does not directly depend on good representati
on.
 Empirical evidence suggest that VAEs are prone to learn uninformative represent
ations while still acheiving low ELBO loss, a degeneration made possible
 by overpowerful decoders that are able to overfit the little information
 captured by the encoder.
 
\begin_inset Formula $D_{KL}\left(e(z|x)\big|\big|m(z)\right)$
\end_inset

 approaches 
\begin_inset Formula $0$
\end_inset

 iff 
\begin_inset Formula $e(z|x)\rightarrow m(z)$
\end_inset

, making 
\begin_inset Formula $e(z|x)$
\end_inset

 close to independence from 
\begin_inset Formula $x$
\end_inset

, resulting in a latent representation that fails to encode information
 about the input.
 However, a suitably powerful decoder could possibly learn to overfit encoded
 traces of the training examples, and reach a low distortion score during
 optimization.
 
\end_layout

\begin_layout Standard
In the current study, we extend this theoretical framework to explain the
 advancements of our proposed loss function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/extended_info_plane.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Phase diagram, a proposed information plane interpretation of VAEs.
 Axes are variational rate and distortion.
 The IB theoretical limit is extended by an additional limit induced by
 the constraint of a finite parametric variational family.
 Once a family is chosen, we seek to learn an optimal marginal 
\begin_inset Formula $m(z)$
\end_inset

 and decoder 
\begin_inset Formula $d(x|z)$
\end_inset

 in order to approach the new limit.
 
\begin_inset Formula $\beta$
\end_inset

 modulation controls the tradeoff between rate and distortion, regardless
 of the variational family.
 Note that this figure is inverted in orientation to Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

, i.e.
 low distortion corresponds to better performance, and not to lower MI.
 Taken from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "phase_diagram"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
IB theory of deep learning
\end_layout

\begin_layout Standard
The following is a summary of work leveraging the IB framework for deterministic
 DNN optimization and interpretation.
 For a more comprehensive review of this opinion-splitting topic, the reader
 is advised to consult the work of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Goldfeld2020}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Tishby2015}
\end_layout

\end_inset

 proposed a representation-learning interpretation of DNNs using the IB
 framework, regarding DNNs as Markov cascades of intermediate representations
 between hidden layers.
 Under this notion, comparing the optimal and the achieved rate-distortion
 ratios between DNN layers will indicate if a model is too complex or too
 simple for a given task and training set.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

 visualized and analyzed the information plane behavior of DNNs over a toy
 problem with a known joint distribution.
 Mutual information of the different layers was estimated and used to analyze
 the training process.
 The learning process over Stochastic Gradient Descent (SGD) exhibited two
 separate and sequential behaviors: A short Empirical Error Minimization
 phase (ERM) characterized by a rapid decrease in distortion, followed by
 a long compression phase with an increase in rate until convergence to
 an optimal IB limit, as demonstrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{shwartz_ziv_info_plane}
\end_layout

\end_inset

.
 Similar, yet repetitive behavior was observed in the current study, as
 elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:text_classification-1}
\end_layout

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/black_box_with_z.png
	scale 43

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Information plane scatters of different DNN layers (colors) in 50 randomized
 networks.
 Left are initial weights, center are at 400 epochs, and right at 9000 epochs.
 Our study reproduced similar, yet repetitive behavior on complicated high
 dimensional tasks, as elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:text_classification-1}
\end_layout

\end_inset

, and in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
 Taken from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "shwartz_ziv_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Saxe2018}
\end_layout

\end_inset

 reproduced the experiments described in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ShwartzZiv2017}
\end_layout

\end_inset

, expanding them to different activation functions, different datasets and
 different methods to estimate mutual information.
 It was found that double-sided saturated nonlinear activations, such as
 the tanh, produced a distinct compressions stage when mutual information
 was measured by binning, as performed in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ShwartzZiv2017}
\end_layout

\end_inset

, while other activations did not.
 It was also shown that DNN generalization did not depend on a distinct
 compression stage, and that DNNs do forget task irrelevant information,
 but this happens concurrently to the learning of task relevant information,
 and not necessarily seperately.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Amjad2020}
\end_layout

\end_inset

 argued against the use of the IB as an objective for deterministic DNNs,
 as mutual information in deterministic DNNs is either infinite or step
 like, because of mutual information's invariance to invertible transformations,
 and because of the absence of a decision function in the objective.
 Using IB as an objective in stochastic DNNs, such as of the variational
 IB family, is suggested as a possible solution.
 
\end_layout

\begin_layout Standard
When examining the information plane behavior in the current study, we notice
 recurring patterns of distortion reduction followed by rate increase, resemblin
g the ERM and representation compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

, as elaborated in Appendix
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:AppendixC}
\end_layout

\end_inset

.

\series bold
 
\end_layout

\begin_layout Subsection*
Conditional Entropy Bottleneck
\end_layout

\begin_layout Standard
As mentioned in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{subsec:variational_approximations}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Fischer2020}
\end_layout

\end_inset

 showed that the conditional entropy bottleneck is equivalent to IB for
 
\begin_inset Formula $\gamma=\beta-1$
\end_inset

 following the chain rule of mutual information 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Cover1999elements}
\end_layout

\end_inset

, and the IB Markov chain.
 We develop this equivalence in detail:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
CEB= & I(X;Z|Y)-\gamma I(Z;Y)\\
\overset{\text{MI chain rule}}{=} & H(Z|Y)-H(Z|X,Y)-\gamma I(Z;Y)\\
\overset{Z\leftarrow X\leftrightarrow Y}{=} & H(Z|Y)-H(Z|X)-\gamma I(Z;Y)\\
\overset{\gamma:=\beta-1}{\Longrightarrow} & H(Z|Y)-H(Z|X)-(\beta-1)I(Z;Y)\\
= & H(Z|Y)-H(Z|X)-\beta I(Z;Y)+I(Z;Y)\\
= & H(Z|Y)-H(Z|X)-\beta I(Z;Y)+H(Z)-H(Z|Y)\\
= & H(Z)-H(Z|X)+H(Z|Y)-H(Z|Y)-\beta I(Z;Y)\\
= & I(X;Z)-\beta I(Z;Y)
\end{align*}

\end_inset


\end_layout

\begin_layout Section*
\start_of_appendix
Appendix C - Experiments elaboration
\begin_inset CommandInset label
LatexCommand label
name "sec:AppendixC"

\end_inset


\end_layout

\begin_layout Standard
Image classification models were trained on the first 500,000 samples of
 the ImageNet 2012 dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{deng2009imagenet}
\end_layout

\end_inset

, and text classification over
\series bold
 
\series default
the entire IMDB
\series bold
 
\series default
sentiment analysis dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{maas2011imdb}
\end_layout

\end_inset

.
 For each dataset, a competitive pre-trained model (Vanilla model) was evaluated
 and then used to encode embeddings.
 These embeddings were then used as a dataset for a new stochastic classifier
 net with either a VIB or a VUB loss function.
 Stochastic classifiers consisted of two ReLU activated linear layers of
 the same dimensions as the pre-trained model's logits (2048 for image and
 768 for text classification), followed by reparameterization and a final
 softmax activated FC layer.
 Learning rate was 
\begin_inset Formula $10^{-4}$
\end_inset

 and decaying exponentially with a factor of 0.97 every two epochs.
 Batch sizes were 32 for ImageNet and 16 for IMDB.
 All models were trained using an Nvidia RTX3080 GPU with approximately
 1-2 days per a single experiment run.
 Beta values of 
\begin_inset Formula $\beta=10^{-i}$
\end_inset

 for 
\begin_inset Formula $i\in\{1,2,3\}$
\end_inset

 were tested, and we used a single forward pass per sample for inference,
 since previous studies indicated that these are the best range and sample
 rate for VIB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Alemi2018}
\end_layout

\end_inset

.
 Each model was trained and evaluated 5 times per 
\begin_inset Formula $\beta$
\end_inset

 value, with consistent performance.
 Statistical significance was demonstrated in all comparisons using the
 Wilcoxon rank sum test with all metrics compared attaining a p-value of
 less than 
\begin_inset Formula $0.05$
\end_inset

.
 Rank sum was computed as follows: A sorted vector of results was prepared
 for each compared metric, where each entry featured the attained result
 in each of the 5 i.i.d.
 experiments per algorithm, and a boolean indicator value for the algorithm
 type.
 For example, let 
\begin_inset Formula $r:=\left((0.94,1)\ (0.935,1)\ (0.93,1)\ (0.93,1)\ (0.925,1)\ (0.92,0)\ (0.915,0)\ (0.915,0)\ (0.91,0)\ (0.89,0)\right)$
\end_inset

 be a sorted vector of (test accuracy, algorithm) tuples, 1 being VUB, 0
 VIB.
 We compute the rank-sum as follows: 
\begin_inset Formula 
\[
\mu_{T}=\frac{5\cdot11}{2}=27.5,\ \sigma_{T}=\sqrt{\frac{5\cdot5\cdot11}{12}}4.78,\ Z(T)=\frac{15-27.5}{4.78}-2.61
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Phi^{-1}(pval)=-2.61,\ pval=0.00450.05
\]

\end_inset


\end_layout

\begin_layout Standard
In practice, these were computed with the Python Scipy library as follows:
\end_layout

\begin_layout LyX-Code
import scipy.stats as stats
\end_layout

\begin_layout LyX-Code
vib_scores = [0.915, 0.915, 0.91, 0.92, 0.89] 
\end_layout

\begin_layout LyX-Code
vub_scores = [0.93, 0.935, 0.925, 0.93, 0.94] 
\end_layout

\begin_layout LyX-Code
pvalue = stats.ranksums(vub_scores, vib_scores, 'greater').pvalue
\end_layout

\begin_layout LyX-Code
assert pvalue < 0.05
\end_layout

\begin_layout Subsection*
Image classification
\end_layout

\begin_layout Standard
The ImageNet 2012 validation set was used for evaluation as the test set
 for ImageNet is unavailable.
 InceptionV3 yields a slightly worse single shot accuracy than inceptionV2
 (80.4%) when run in a single model and single crop setting, however we've
 used InceptionV3 over V2 for simplicity.
 Each model was trained for 100 epochs.
 The entire validation set was used to measure accuracy and robustness to
 FGS attacks, while only 1% of it was used for CW attacks, as they are computati
onally expensive.
 Examples of successful attacks are shown in Figures
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:untargeted_examples},
\backslash
ref{fig:targeted_examples}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/image_attacks/untargeted_attacks_oneline.png
	scale 26.5

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful untargeted FGS attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 Perturbation magnitude is determined by the parameter 
\begin_inset Formula $\epsilon$
\end_inset

 shown on the left, the higher, the more perturbed.
 Original and wrongly assigned labels are listed at the top of each image.
 Notice the deterioration of image quality as 
\begin_inset Formula $\epsilon$
\end_inset

 increases.
\begin_inset CommandInset label
LatexCommand label
name "fig:untargeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/image_attacks/targeted_attacks.png
	scale 46.5

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful targeted CW attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 The target label is 'Soccer ball'.
 Average 
\begin_inset Formula $L_{2}$
\end_inset

 distance required for a successful attack is shown on the left.
 The higher the required 
\begin_inset Formula $L_{2}$
\end_inset

 distance, the greater the visible change required to fool the model.
 Original and wrongly assigned labels are listed at the top of each image.
 Mind the difference in noticeable change as compared to the FGS perturbations
 presented in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:untargeted_examples}
\end_layout

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:targeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/info_plane_imdb.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated information plane metrics per epoch for VUB trained on IMDB with
 
\begin_inset Formula $\beta=0.001$
\end_inset

.
 
\begin_inset Formula $I(X;Z)$
\end_inset

 is approximated by 
\begin_inset Formula $H(R)-H(Z|X)$
\end_inset

 and 
\begin_inset Formula $\frac{1}{CE(Y;\hat{Y})}$
\end_inset

 is used as an analog for 
\begin_inset Formula $I(Z;Y)$
\end_inset

.
 The epochs have been grouped and color-coded in intervals of 30 epochs
 in the order: Orange (0-30), gray (30-60), yellow (60-90), green (90-120)
 and red (120-150).
 We notice recurring patterns of distortion reduction followed by rate increase,
 resembling the ERM and representation compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "estimated_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Text classification
\begin_inset CommandInset label
LatexCommand label
name "sec:text_classification-1"

\end_inset


\end_layout

\begin_layout Standard
Each model was trained for 150 epochs.
 The entire test set was used to measure accuracy, while only the first
 200 entries in the test set were used for adversarial attacks, as they
 are computationally expensive.
 Examples of successful attacks are shown in Tables
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:pwws_examples},
\backslash
ref{tab:text_attack_examples}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="1">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="8cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Original text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
the acting , costumes , music , cinematography and sound are all 
\emph on
astounding
\emph default
 given the production 's austere locales .
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Perturbed text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
the acting , costumes , music , cinematography and sound are all 
\emph on
dumbfounding
\emph default
 given the production 's austere locales .
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example of a successful PWWS attack on a vanilla Bert model, fine tuned
 over the IMDB dataset.
 The original label is 'Positive sentiment'.
 The substituted word, marked in italic font, changed the classification
 to 'Negative sentiment'.
 VUB and VIB classifiers are far less susceptible to these perturbations
 as shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:pwws_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="1">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="8cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Original text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\emph on
great
\emph default
 historical movie, will not allow a viewer to leave once you begin to watch.
 View is presented differently than displayed by most school books on this
 
\emph on
subject
\emph default
.
 My only fault for this movie is it was photographed in black and white;
 wished it had been in color ...
 wow !
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Perturbed text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\emph on
gnreat
\emph default
 historical movie, will not allow a viewer to leave once you begin to watch.
 View is presented differently than displayed by most school books on this
 
\emph on
sSbject
\emph default
.
 My only fault for this movie is it was photographed in black and white;
 wished it had been in color ...
 wow !
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example of a successful Deep Word Bug attack on a vanilla Bert model, fine
 tuned over the IMDB dataset.
 The original label is 'Positive sentiment'.
 Perturbations, marked in italic font, change the classification to 'Negative
 sentiment'.
 VUB and VIB classifiers are far less susceptible to these perturbations,
 as shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In addition to the above evaluation metrics, we also measured approximated
 rate and distortion throughout text classification training, and plotted
 them on the information plane as shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
 Examining the resulting curve, we notice recurring patterns of distortion
 reduction followed by rate increase, resembling the ERM and representation
 compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

, suggesting that interesting information plane patterns can occur in high
 dimensional tasks, opening the door to possible future research.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage 
\backslash
section*{NeurIPS Paper Checklist}
\end_layout

\begin_layout Plain Layout


\backslash
begin{enumerate}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Claims}     
\backslash
item[] Question: Do the main claims made in the abstract and introduction
 accurately reflect the paper's contributions and scope?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: We believe our abstract and intro are well aligned
 with the main body of the paper.
      
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the abstract and introduction do not include
 the claims made in the paper.
         
\backslash
item The abstract and/or introduction should clearly state the claims made,
 including the contributions made in the paper and important assumptions
 and limitations.
 A No or NA answer to this question will not be perceived well by the reviewers.
          
\backslash
item The claims made should match theoretical and experimental results,
 and reflect how much the results can be expected to generalize to other
 settings.
          
\backslash
item It is fine to include aspirational goals as motivation as long as it
 is clear that these goals are not attained by the paper.
      
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Limitations}     
\backslash
item[] Question: Does the paper discuss the limitations of the work performed
 by the authors?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: We clearly state experiments where we do not outperform
 previous work in Section
\backslash
ref{sec:Experiments}, and provide alterative hypothesis for our claim for
 better generalization, and define the assumptions underlying our claim,
 in Section
\backslash
ref{sec:Discussion}.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper has no limitation while the answer
 No means that the paper has limitations, but those are not discussed in
 the paper.
          
\backslash
item The authors are encouraged to create a separate "Limitations" section
 in their paper.
         
\backslash
item The paper should point out any strong assumptions and how robust the
 results are to violations of these assumptions (e.g., independence assumptions,
 noiseless settings, model well-specification, asymptotic approximations
 only holding locally).
 The authors should reflect on how these assumptions might be violated in
 practice and what the implications would be.
         
\backslash
item The authors should reflect on the scope of the claims made, e.g., if
 the approach was only tested on a few datasets or with a few runs.
 In general, empirical results often depend on implicit assumptions, which
 should be articulated.
         
\backslash
item The authors should reflect on the factors that influence the performance
 of the approach.
 For example, a facial recognition algorithm may perform poorly when image
 resolution is low or images are taken in low lighting.
 Or a speech-to-text system might not be used reliably to provide closed
 captions for online lectures because it fails to handle technical jargon.
         
\backslash
item The authors should discuss the computational efficiency of the proposed
 algorithms and how they scale with dataset size.
         
\backslash
item If applicable, the authors should discuss possible limitations of their
 approach to address problems of privacy and fairness.
         
\backslash
item While the authors might fear that complete honesty about limitations
 might be used by reviewers as grounds for rejection, a worse outcome might
 be that reviewers discover limitations that aren't acknowledged in the
 paper.
 The authors should use their best judgment and recognize that individual
 actions in favor of transparency play an important role in developing norms
 that preserve the integrity of the community.
 Reviewers will be specifically instructed to not penalize honesty concerning
 limitations.
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Theory Assumptions and Proofs}     
\backslash
item[] Question: For each theoretical result, does the paper provide the
 full set of assumptions and a complete (and correct) proof?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: We provide a clear proof for our claim of a tighter
 bound on the IB objective in Section
\backslash
ref{sec:vub}, and accompany it with a rigorous analysis of prior work both
 in Section
\backslash
ref{sec:related_work}, and in AppendixB.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not include theoretical results.
          
\backslash
item All the theorems, formulas, and proofs in the paper should be numbered
 and cross-referenced.
         
\backslash
item All assumptions should be clearly stated or referenced in the statement
 of any theorems.
         
\backslash
item The proofs can either appear in the main paper or the supplemental
 material, but if they appear in the supplemental material, the authors
 are encouraged to provide a short proof sketch to provide intuition.
          
\backslash
item Inversely, any informal proof provided in the core of the paper should
 be complemented by formal proofs provided in appendix or supplemental material.
         
\backslash
item Theorems and Lemmas that the proof relies upon should be properly reference
d.
      
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout

    
\backslash
item {
\backslash
bf Experimental Result Reproducibility}     
\backslash
item[] Question: Does the paper fully disclose all the information needed
 to reproduce the main experimental results of the paper to the extent that
 it affects the main claims and/or conclusions of the paper (regardless
 of whether the code and data are provided or not)?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: Experiments are provided in full detail in Section
\backslash
ref{sec:Experiments} and in AppendixC.
 Provided code is easy to run and is well documented.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not include experiments.
         
\backslash
item If the paper includes experiments, a No answer to this question will
 not be perceived well by the reviewers: Making the paper reproducible is
 important, regardless of whether the code and data are provided or not.
         
\backslash
item If the contribution is a dataset and/or model, the authors should describe
 the steps taken to make their results reproducible or verifiable.
          
\backslash
item Depending on the contribution, reproducibility can be accomplished
 in various ways.
 For example, if the contribution is a novel architecture, describing the
 architecture fully might suffice, or if the contribution is a specific
 model and empirical evaluation, it may be necessary to either make it possible
 for others to replicate the model with the same dataset, or provide access
 to the model.
 In general.
 releasing code and data is often one good way to accomplish this, but reproduci
bility can also be provided via detailed instructions for how to replicate
 the results, access to a hosted model (e.g., in the case of a large language
 model), releasing of a model checkpoint, or other means that are appropriate
 to the research performed.
         
\backslash
item While NeurIPS does not require releasing code, the conference does
 require all submissions to provide some reasonable avenue for reproducibility,
 which may depend on the nature of the contribution.
 For example         
\backslash
begin{enumerate}             
\backslash
item If the contribution is primarily a new algorithm, the paper should
 make it clear how to reproduce that algorithm.
             
\backslash
item If the contribution is primarily a new model architecture, the paper
 should describe the architecture clearly and fully.
             
\backslash
item If the contribution is a new model (e.g., a large language model), then
 there should either be a way to access this model for reproducing the results
 or a way to reproduce the model (e.g., with an open-source dataset or instruction
s for how to construct the dataset).
             
\backslash
item We recognize that reproducibility may be tricky in some cases, in which
 case authors are welcome to describe the particular way they provide for
 reproducibility.
 In the case of closed-source models, it may be that access to the model
 is limited in some way (e.g., to registered users), but it should be possible
 for other researchers to have some path to reproducing or verifying the
 results.
         
\backslash
end{enumerate}     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Open access to data and code}     
\backslash
item[] Question: Does the paper provide open access to the data and code,
 with sufficient instructions to faithfully reproduce the main experimental
 results, as described in supplemental material?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: The provided code runs easily using a virtual Python
 environmet, with a complete README.md file and in code documentation.
 The datasets used are well known and public datasets.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that paper does not include experiments requiring
 code.
         
\backslash
item Please see the NeurIPS code and data submission guidelines (
\backslash
url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
         
\backslash
item While we encourage the release of code and data, we understand that
 this might not be possible, so No is an acceptable answer.
 Papers cannot be rejected simply for not including code, unless this is
 central to the contribution (e.g., for a new open-source benchmark).
         
\backslash
item The instructions should contain the exact command and environment needed
 to run to reproduce the results.
 See the NeurIPS code and data submission guidelines (
\backslash
url{https://nips.cc/public/guides/CodeSubmissionPolicy}) for more details.
         
\backslash
item The authors should provide instructions on data access and preparation,
 including how to access the raw data, preprocessed data, intermediate data,
 and generated data, etc.
         
\backslash
item The authors should provide scripts to reproduce all experimental results
 for the new proposed method and baselines.
 If only a subset of experiments are reproducible, they should state which
 ones are omitted from the script and why.
         
\backslash
item At submission time, to preserve anonymity, the authors should release
 anonymized versions (if applicable).
         
\backslash
item Providing as much information as possible in supplemental material
 (appended to the paper) is recommended, but including URLs to data and
 code is permitted.
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Experimental Setting/Details}     
\backslash
item[] Question: Does the paper specify all the training and test details
 (e.g., data splits, hyperparameters, how they were chosen, type of optimizer,
 etc.) necessary to understand the results?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: AppendixC provides a thorough drill down of the experimen
tal process, including handling of data, hyperparameters and architecture.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not include experiments.
         
\backslash
item The experimental setting should be presented in the core of the paper
 to a level of detail that is necessary to appreciate the results and make
 sense of them.
         
\backslash
item The full details can be provided either with the code, in appendix,
 or as supplemental material.
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Experiment Statistical Significance}     
\backslash
item[] Question: Does the paper report error bars suitably and correctly
 defined or other appropriate information about the statistical significance
 of the experiments?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: Each experiment was run 5 times with mean and std
 deviation reported in detail.
 Statistical significance was assured using a Wilcoxon rank sum test as
 elaborated in AppendixC.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not include experiments.
         
\backslash
item The authors should answer "Yes" if the results are accompanied by error
 bars, confidence intervals, or statistical significance tests, at least
 for the experiments that support the main claims of the paper.
         
\backslash
item The factors of variability that the error bars are capturing should
 be clearly stated (for example, train/test split, initialization, random
 drawing of some parameter, or overall run with given experimental conditions).
         
\backslash
item The method for calculating the error bars should be explained (closed
 form formula, call to a library function, bootstrap, etc.)         
\backslash
item The assumptions made should be given (e.g., Normally distributed errors).
         
\backslash
item It should be clear whether the error bar is the standard deviation
 or the standard error of the mean.
         
\backslash
item It is OK to report 1-sigma error bars, but one should state it.
 The authors should preferably report a 2-sigma error bar than state that
 they have a 96
\backslash
% CI, if the hypothesis of Normality of errors is not verified.
         
\backslash
item For asymmetric distributions, the authors should be careful not to
 show in tables or figures symmetric error bars that would yield results
 that are out of range (e.g.
 negative error rates).
         
\backslash
item If error bars are reported in tables or plots, The authors should explain
 in the text how they were calculated and reference the corresponding figures
 or tables in the text.
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Experiments Compute Resources}     
\backslash
item[] Question: For each experiment, does the paper provide sufficient
 information on the computer resources (type of compute workers, memory,
 time of execution) needed to reproduce the experiments?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: The type of GPU and training time is elaborated in
 AppendixC.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not include experiments.
         
\backslash
item The paper should indicate the type of compute workers CPU or GPU, internal
 cluster, or cloud provider, including relevant memory and storage.
         
\backslash
item The paper should provide the amount of compute required for each of
 the individual experimental runs as well as estimate the total compute.
          
\backslash
item The paper should disclose whether the full research project required
 more compute than the experiments reported in the paper (e.g., preliminary
 or failed experiments that didn't make it into the paper).
      
\backslash
end{itemize}      
\backslash
item {
\backslash
bf Code Of Ethics}     
\backslash
item[] Question: Does the research conducted in the paper conform, in every
 respect, with the NeurIPS Code of Ethics 
\backslash
url{https://neurips.cc/public/EthicsGuidelines}?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: We've reviewed the NeurIPS code of ethics and assured
 our research does not breach it.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the authors have not reviewed the NeurIPS
 Code of Ethics.
         
\backslash
item If the authors answer No, they should explain the special circumstances
 that require a deviation from the Code of Ethics.
         
\backslash
item The authors should make sure to preserve anonymity (e.g., if there is
 a special consideration due to laws or regulations in their jurisdiction).
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Broader Impacts}     
\backslash
item[] Question: Does the paper discuss both potential positive societal
 impacts and negative societal impacts of the work performed?     
\backslash
item[] Answer: 
\backslash
answerNA{}     
\backslash
item[] Justification: Our research focuses on learning theory and does not
 have any societal impact.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that there is no societal impact of the work performed.
         
\backslash
item If the authors answer NA or No, they should explain why their work
 has no societal impact or why the paper does not address societal impact.
         
\backslash
item Examples of negative societal impacts include potential malicious or
 unintended uses (e.g., disinformation, generating fake profiles, surveillance),
 fairness considerations (e.g., deployment of technologies that could make
 decisions that unfairly impact specific groups), privacy considerations,
 and security considerations.
         
\backslash
item The conference expects that many papers will be foundational research
 and not tied to particular applications, let alone deployments.
 However, if there is a direct path to any negative applications, the authors
 should point it out.
 For example, it is legitimate to point out that an improvement in the quality
 of generative models could be used to generate deepfakes for disinformation.
 On the other hand, it is not needed to point out that a generic algorithm
 for optimizing neural networks could enable people to train models that
 generate Deepfakes faster.
         
\backslash
item The authors should consider possible harms that could arise when the
 technology is being used as intended and functioning correctly, harms that
 could arise when the technology is being used as intended but gives incorrect
 results, and harms following from (intentional or unintentional) misuse
 of the technology.
         
\backslash
item If there are negative societal impacts, the authors could also discuss
 possible mitigation strategies (e.g., gated release of models, providing
 defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms
 to monitor how a system learns from feedback over time, improving the efficienc
y and accessibility of ML).
     
\backslash
end{itemize}      
\backslash
item {
\backslash
bf Safeguards}     
\backslash
item[] Question: Does the paper describe safeguards that have been put in
 place for responsible release of data or models that have a high risk for
 misuse (e.g., pretrained language models, image generators, or scraped datasets)?
     
\backslash
item[] Answer: 
\backslash
answerNA{}     
\backslash
item[] Justification: Our paper does not release any proprietary data or
 high risk model.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper poses no such risks.
         
\backslash
item Released models that have a high risk for misuse or dual-use should
 be released with necessary safeguards to allow for controlled use of the
 model, for example by requiring that users adhere to usage guidelines or
 restrictions to access the model or implementing safety filters.
          
\backslash
item Datasets that have been scraped from the Internet could pose safety
 risks.
 The authors should describe how they avoided releasing unsafe images.
         
\backslash
item We recognize that providing effective safeguards is challenging, and
 many papers do not require this, but we encourage authors to take this
 into account and make a best faith effort.
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Licenses for existing assets}     
\backslash
item[] Question: Are the creators or original owners of assets (e.g., code,
 data, models), used in the paper, properly credited and are the license
 and terms of use explicitly mentioned and properly respected?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: Every dataset and code repository used are of an open
 license and are duely cited.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not use existing assets.
         
\backslash
item The authors should cite the original paper that produced the code package
 or dataset.
         
\backslash
item The authors should state which version of the asset is used and, if
 possible, include a URL.
         
\backslash
item The name of the license (e.g., CC-BY 4.0) should be included for each
 asset.
         
\backslash
item For scraped data from a particular source (e.g., website), the copyright
 and terms of service of that source should be provided.
         
\backslash
item If assets are released, the license, copyright information, and terms
 of use in the package should be provided.
 For popular datasets, 
\backslash
url{paperswithcode.com/datasets} has curated licenses for some datasets.
 Their licensing guide can help determine the license of a dataset.
         
\backslash
item For existing datasets that are re-packaged, both the original license
 and the license of the derived asset (if it has changed) should be provided.
         
\backslash
item If this information is not available online, the authors are encouraged
 to reach out to the asset's creators.
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf New Assets}     
\backslash
item[] Question: Are new assets introduced in the paper well documented
 and is the documentation provided alongside the assets?     
\backslash
item[] Answer: 
\backslash
answerYes{}     
\backslash
item[] Justification: The code released with the paper is well documented
 both with a clear README.md file, and with in code documentation.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not release new assets.
         
\backslash
item Researchers should communicate the details of the dataset/code/model
 as part of their submissions via structured templates.
 This includes details about training, license, limitations, etc.
          
\backslash
item The paper should discuss whether and how consent was obtained from
 people whose asset is used.
         
\backslash
item At submission time, remember to anonymize your assets (if applicable).
 You can either create an anonymized URL or include an anonymized zip file.
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Crowdsourcing and Research with Human Subjects}     
\backslash
item[] Question: For crowdsourcing experiments and research with human subjects,
 does the paper include the full text of instructions given to participants
 and screenshots, if applicable, as well as details about compensation (if
 any)?      
\backslash
item[] Answer: 
\backslash
answerNA{}     
\backslash
item[] Justification: Our paper does not involve crowdsourcing nor research
 with human subjects.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not involve crowdsourcing nor
 research with human subjects.
         
\backslash
item Including this information in the supplemental material is fine, but
 if the main contribution of the paper involves human subjects, then as
 much detail as possible should be included in the main paper.
          
\backslash
item According to the NeurIPS Code of Ethics, workers involved in data collectio
n, curation, or other labor should be paid at least the minimum wage in
 the country of the data collector.
      
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
item {
\backslash
bf Institutional Review Board (IRB) Approvals or Equivalent for Research
 with Human Subjects}     
\backslash
item[] Question: Does the paper describe potential risks incurred by study
 participants, whether such risks were disclosed to the subjects, and whether
 Institutional Review Board (IRB) approvals (or an equivalent approval/review
 based on the requirements of your country or institution) were obtained?
     
\backslash
item[] Answer: 
\backslash
answerNA{}     
\backslash
item[] Justification: Our paper does not involve crowdsourcing nor research
 with human subjects.
     
\backslash
item[] Guidelines:     
\backslash
begin{itemize}         
\backslash
item The answer NA means that the paper does not involve crowdsourcing nor
 research with human subjects.
         
\backslash
item Depending on the country in which research is conducted, IRB approval
 (or equivalent) may be required for any human subjects research.
 If you obtained IRB approval, you should clearly state this in the paper.
          
\backslash
item We recognize that the procedures for this may vary significantly between
 institutions and locations, and we expect authors to adhere to the NeurIPS
 Code of Ethics and the guidelines for their institution.
          
\backslash
item For initial submissions, do not include any information that would
 break anonymity (if applicable), such as the institution conducting the
 review.
     
\backslash
end{itemize}
\end_layout

\begin_layout Plain Layout


\backslash
end{enumerate}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
