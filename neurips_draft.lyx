#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
%\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

\PassOptionsToPackage{verbose=true,letterpaper}{geometry}
\PassOptionsToPackage{utf8}{inputenc}

% ready for submission
\usepackage{neurips_2024}

\usepackage{natbib}
\AtBeginDocument{%
  \@ifpackageloaded{natbib}%
    {%
      % When natbib is in use, set the proper style and fix a few things
      \let\cite\citep
      \let\shortcite\citeyearpar
      \setcitestyle{aysep={}}
      \setlength\bibhang{0pt}
      \bibliographystyle{aaai24}
    }{}%
}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%\title{Formatting Instructions For NeurIPS 2024}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{Nir Weingarten, Moshe Butman \& Zohar Yakhini \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Reichman University\\
Herzliya, Israel \\
\texttt{\{nir.weingarten,zohar.yakhini,moshe.butman\}@runi.ac.il} \\
\And
Ran Gilad-Bachrach \\
Department of Computational Biology \\
Tel-Aviv University \\
Tel-Aviv, Israel \\
\texttt{rgb@tau.ac.il} \\
\And
Ronit Bustin \\
Advanced Technical Center \\
General Motors Research \\
Herzliya, Israel \\
\texttt{ronit.bustin@gmail.com} \\}


% roman numerals
\renewcommand{\theequation}{\roman{equation}}

%\bibliographystyle{../iclr_vub_paper/iclr2024_conference}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format pdf2
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing onehalf
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\rightmargin 5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
A Tighter Bound on the Information Bottleneck with Applications to Deep
 Learning
\end_layout

\begin_layout Abstract
The Information Bottleneck, IB, provides a hypothetically optimal framework
 for unsupervised data modeling, yet is often intractable.
 Recent efforts optimized supervised DNNs with a variational upper bound
 to the IB objective, resulting in improved robustness to adversarial attacks.
 However, when deriving the upper bound, it is presumed that the supervisor
 distribution 
\begin_inset Formula $p^{*}(y)$
\end_inset

 is known, as it would be in the unsupervised case.
 This work demonstrates that lifting this assumption not only results in
 a tighter bound on the IB and improved empirical performance, but also
 proposes a new motivation for regularization.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Deep Neural Nets (DNNs) learn latent representations induced by their downstream
 task, objective function, and other parameters.
 The quality of the learned representations impacts the DNN's generalization
 ability and the coherence of the emerging latent space 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Bengio2009}
\end_layout

\end_inset

.
 A question emerges regarding the extraction of an optimal latent representation
 for all data points from a restricted set of training examples.
 Classic information theory provides rate-distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Shannon1959}
\end_layout

\end_inset

 for optimal compression of data.
 However, rate-distortion regards all information as equal, not taking into
 account which information is more relevant to a specified downstream task,
 without constructing tailored distortion functions.
 The Information Bottleneck, IB, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 resolves this limitation by defining mutual information, MI, between the
 learned representation and a designated downstream task as a universal
 distortion function.
 Yet, learning representations using the IB method requires computations
 of mutual information, which is possible between discrete distributions,
 and some continuous ones, but not in the general case 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Chechik2003}
\end_layout

\end_inset

.
 Moreover, MI is either difficult or impossible to optimize over when using
 deterministic models 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Saxe2018,Amjad2020}
\end_layout

\end_inset

.
 Nonetheless, the promise of the IB remains alluring, and recents works
 utilized VAE 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 inspired variational approximations to approximate upper bounds to the
 IB objective, allowing its utilization as a loss function for DNNs, where
 the underlying distributions are both continuous and unknown 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Fischer2020,Cheng2020}
\end_layout

\end_inset

.
 These approaches learn representations in supervised settings, without
 knowledge of the underlying distribution 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

, utilizing the learned variational conditional 
\begin_inset Formula $p(y|x)$
\end_inset

 to approximate MI.
 In contrast, non variational IB methods learn representations in unsupervised
 settings, where the stochastic process underlying the observed data is
 known 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999,Chechik2003,Painsky2017}
\end_layout

\end_inset

.
 Nonetheless, when deriving the variational IB objectives, previous research
 relax the problem by assuming that 
\begin_inset Formula $p^{*}(y)$
\end_inset

 is constant, while in practice it's an optimized variational approximation.
 We derive a new upper bound for the IB objective, and a subsequent variational
 approximation, by removing the relaxation.
 We show that our bound is tighter than previous bounds, and that our proposed
 loss function is a tighter variational approximation, when considering
 
\begin_inset Formula $p(y|x)$
\end_inset

 as part of the optimization.
 We believe our new derivation is a better adaptation of the IB for supervised
 tasks, and show empirical evidence of improved performance across several
 challenging tasks over different modalities.
 We utilize previous studies on variational representation learning and
 regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018,Pereyra2017,Achille2018}
\end_layout

\end_inset

 to interpret our findings, and conclude that our proposed derivation applies
 regularization over the variational classifier, preventing it from overfitting
 the learned representations and thus enabling greater MI between learned
 representation and the real and unknown stochastic process.
 
\end_layout

\begin_layout Standard
The reader is encouraged to refer to the preliminaries provided in Appendix
\begin_inset space ~
\end_inset

A before proceeding.
\end_layout

\begin_layout Section
Related work
\begin_inset CommandInset label
LatexCommand label
name "sec:related_work"

\end_inset


\end_layout

\begin_layout Subsection
Deterministic Information Bottleneck
\begin_inset CommandInset label
LatexCommand label
name "sec:deterministic_ib"

\end_inset


\end_layout

\begin_layout Standard
Classic information theory offers rate-distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Shannon1959}
\end_layout

\end_inset

 to mitigate signal loss during compression: A source 
\begin_inset Formula $X$
\end_inset

 is compressed to an encoding 
\begin_inset Formula $Z$
\end_inset

, such that maximal compression is achieved while keeping the encoding quality
 above a certain threshold.
 Encoding quality is measured by a task specific distortion function: 
\begin_inset Formula $d:X\times Z\mapsto\mathbb{R}^{+}$
\end_inset

.
 Rate-distortion suggests a mapping that minimizes the rate of bits to source
 sample, measured by 
\begin_inset Formula $I(X;Z)$
\end_inset

, that adheres to a chosen allowed expected distortion 
\begin_inset Formula $D\ge0$
\end_inset

.
 The Information Bottleneck, IB, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 extends rate-distortion by replacing the tailored distortion functions
 with MI over a target distribution: Let 
\begin_inset Formula $Y$
\end_inset

 be the target signal for some specific downstream task, such that the joint
 distribution 
\begin_inset Formula $P^{*}(x,y)$
\end_inset

 is known, and define the distortion function as MI between 
\begin_inset Formula $Z$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
 The IB is the solution to the optimization problem 
\begin_inset Formula $Z:\underset{P(Z|X)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;Y)\ge D$
\end_inset

, that can be optimized by minimizing the IB objective 
\begin_inset Formula $\mathcal{L}_{IB}=I(X;Z)-\beta I(Z;Y)$
\end_inset

 over 
\begin_inset Formula $P(Z|X)$
\end_inset

.
 The solution to the IB objective is a function of the Lagrange multiplier
 
\begin_inset Formula $\beta$
\end_inset

, and is a theoretical limit for representation quality, given mutual informatio
n as an accepted metric, as elaborated in more detail in Appendix
\begin_inset space ~
\end_inset

B.
 The IB is in fact an unsupervised soft clustering problem, where each data
 point 
\begin_inset Formula $x$
\end_inset

 is assigned a probabilities to belong to the different clusters 
\begin_inset Formula $z$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Slonim2002}
\end_layout

\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Chechik2003}
\end_layout

\end_inset

 showed that computing the IB for continuous distributions is hard in the
 general case, and provided a method to optimize the IB objective in the
 case where 
\begin_inset Formula $X,Y$
\end_inset

 are known and jointly Gaussian.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Painsky2017}
\end_layout

\end_inset

 offered a limited linear approximation of the IB for any distribution by
 extracting the jointly gaussian element of given distributions.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Saxe2018}
\end_layout

\end_inset

 considered the application of the IB functional as an objective for DNNs,
 and concluded that computing mutual information in deterministic DNNs is
 problematic as the entropy term 
\begin_inset Formula $H(Z|X)$
\end_inset

 for a continuous 
\begin_inset Formula $Z$
\end_inset

 is infinite.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Amjad2020}
\end_layout

\end_inset

 extended this observation and pointed out that for a discrete 
\begin_inset Formula $Z$
\end_inset

 MI becomes a piecewise constant function of its parameters, making gradient
 descent limiting and difficult.
\end_layout

\begin_layout Subsection
Variational Information Bottleneck
\begin_inset CommandInset label
LatexCommand label
name "subsec:variational_approximations"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 introduced the Variational Information Bottleneck (VIB) - a variational
 approximation for an upper bound to the IB objective for DNN optimization.
 Bounds for 
\begin_inset Formula $I(X,Z)$
\end_inset

 and 
\begin_inset Formula $I(Z,Y)$
\end_inset

 are derived from the non negativity of KL divergence, and are used to form
 an upper bound for the IB objective.
 A variational upper bound is derived by replacing intractable distributions
 with variational approximations.
 Using the 'reparameterization trick' 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 a discrete empirical estimation of the variational upper bound is then
 used as a loss function for classifier DNN optimization.
 The subsequent loss function is equivalent to the 
\begin_inset Formula $\beta$
\end_inset

-autoencoder loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Higgins2017}
\end_layout

\end_inset

.
 VIB was evaluated over image classification tasks, and displayed substantial
 improvements in robustness to adversarial attacks, while causing a slight
 reduction in test set accuracy, comparing to equivalent deterministic models.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Achille2018}
\end_layout

\end_inset

 extended VIB with a total correlation term, designed to increase latent
 disentanglement.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Fischer2020}
\end_layout

\end_inset

 proposed an IB based loss function named Conditional Entropy Bottleneck
 (CEB), in which the conditional mutual information of 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Z$
\end_inset

 given 
\begin_inset Formula $Y$
\end_inset

 is minimized, instead of the unconditional mutual information.
 The CEB loss, 
\begin_inset Formula $L_{CEB}=\underset{Z}{min}I(X;Z|Y)-\gamma I(Y;Z)$
\end_inset

, is designed to minimize all information in 
\begin_inset Formula $Z$
\end_inset

 that is not relevant to the downstream task 
\begin_inset Formula $Y$
\end_inset

, by conditioning over 
\begin_inset Formula $Y$
\end_inset

.
 Similarly to VIB, a variational approximation for CEB was proposed as 
\begin_inset Formula $L_{VCEB}=\mathbb{E}_{x,y}log\left(e(z|x)\right)-\mathbb{E}_{x,y}log\left(b(z|y)\right)-\gamma\mathbb{E}_{x,y}log\left(c(y|z)\right)$
\end_inset

 and tested over the FMNIST 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Xiao2017fashionmnist}
\end_layout

\end_inset

 and CIFAR10 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Krizhevsky2009learning}
\end_layout

\end_inset

 datasets.
 CEB was shown to be equivalent to IB for 
\begin_inset Formula $\gamma=\beta-1$
\end_inset

 following the chain rule of mutual information and the IB Markov chain,
 as established in Appendix
\begin_inset space ~
\end_inset

B.
 VIB was shown to be a special case of VCEB, where rate is approximated
 by the variational expression 
\begin_inset Formula $\mathbb{E}_{x,y}log\left(e(z|x)\right)-\mathbb{E}_{x,y}log\left(b(z|y)\right)$
\end_inset

 instead of 
\begin_inset Formula $\mathbb{E}_{x,y}log\left(e(z|x)\right)-\mathbb{E}_{x,y}log\left(r(z)\right)$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Geiger2020}
\end_layout

\end_inset

 investigated wether VCEB is a tighter variational approximation to IB than
 VIB, and concluded that no ordering can be established in the general case,
 noting that any empirical improvement VCEB exhibits over VIB is not due
 to a tighter variational bound on the IB, but rather of VCEB being more
 amenable to optimization, or simply a successful lost function in its own
 regard.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Cheng2020}
\end_layout

\end_inset

 proposed CLUB, an upper bound based MI estimator, that empirically outperformed
 the popular MINE estimator 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Belghazi2018}
\end_layout

\end_inset

.
 Since CLUB is an upper bound for MI, it was evaluated as a replacement
 to the upper bound for the IB rate term, 
\begin_inset Formula $I(X;Z)$
\end_inset

, proposed in VIB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017}
\end_layout

\end_inset

.
 CLUB based VIB was tested it the over the MNIST dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Deng2012mnist}
\end_layout

\end_inset

, resulting in a slight improvement in accuracy compared to VIB, without
 reporting adversarial robustness.
 We note that CLUB does not prove a tighter bound on the VIB rate term ,
 or on the IB objective.
 We also note, that our current work derives a tighter bound on IB through
 the IB distortion term, 
\begin_inset Formula $I(Z;Y)$
\end_inset

, and that combining our suggested method with a CLUB bound on rate is an
 interesting avenue for future work.
\end_layout

\begin_layout Subsection
Information theoretic regularization
\begin_inset CommandInset label
LatexCommand label
name "sec:it_regularization"

\end_inset


\end_layout

\begin_layout Standard
Label smoothing 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 and entropy regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

 both regularize classifier DNNs by increasing the entropy of their output.
 This is done either directly by inserting a scaled conditional entropy
 term to the loss function, 
\begin_inset Formula $-\gamma\cdot H\left(p_{\theta}(y|x)\right)$
\end_inset

, or by smoothing the training data labels.
 Applying both methods was demonstrated to improve test accuracy and model
 calibration on various challenging classification tasks.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 distinguishes between the IB theoretical limit for representation quality,
 and the limits of variational approximations in VAEs.
 The variational limit is looser, and depends on the chosen variational
 family.
 In the context of VAEs, the representation quality depends on choosing
 marginal and decoder distributions that are close to the true distributions.
 A common degeneration is the usage of overpowerful decoders that overfit
 encoder embeddings, inducing low quality latent representations.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 claims that the VAE ELBO loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 is susceptible to this flaw, as its KL regularization term isn't directly
 affected by the quality of the reconstructed image, and no additional regulariz
ation is performed over the decoder directly.
 This work is further elaborated on in Appendix
\begin_inset space ~
\end_inset

B.
 In the current study, a conditional entropy term 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

 emerges from our proposed derivation of a variational IB loss function,
 and we extend the theoretic framework proposed in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

 to interpret why this new term facilitates a better variational approximation
 of the IB objective.
\end_layout

\begin_layout Section
From VIB to VUB
\begin_inset CommandInset label
LatexCommand label
name "sec:vub"

\end_inset


\end_layout

\begin_layout Standard
As elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:deterministic_ib}
\end_layout

\end_inset

, IB optimization is defined as 
\begin_inset Formula $Z:\underset{P(Z|X)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;Y)\ge D$
\end_inset

.
 While this is true in the unsupervised case, adapting IB to supervised
 tasks admits the learned classifier as a new RV to the optimization problem.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Geiger2020}
\end_layout

\end_inset

 suggested the Markov chain 
\begin_inset Formula $Y\leftrightarrow X\text{\ensuremath{\rightarrow}}Z\rightarrow\hat{Y}$
\end_inset

 for variational IB, distinguishing between the real unknown RV 
\begin_inset Formula $Y$
\end_inset

, and the variational approximation 
\begin_inset Formula $\hat{Y}$
\end_inset

.
 Following this logic, variational IB optimization should be defined as:
 
\begin_inset Formula $Z,\hat{Y}:\underset{P(Z|X),P(\hat{Y}|Z)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;\hat{Y})\ge D$
\end_inset

.
 Previous research had relaxed the supervised optimization problem, in order
 to derive a tractable loss function.
 The VIB loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017}
\end_layout

\end_inset

 consists of a cross entropy (
\begin_inset Formula $CE$
\end_inset

) term and a beta regulated KL regularization term, as in 
\begin_inset Formula $\beta$
\end_inset

-VAE loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Higgins2017}
\end_layout

\end_inset

.
 The KL term is derived from a bound on the IB rate term 
\begin_inset Formula $I(X;Z)$
\end_inset

, while the 
\begin_inset Formula $CE$
\end_inset

 term is derived from a bound on the IB distortion term 
\begin_inset Formula $I(Z;Y)=H(Y)-H(Y|Z)$
\end_inset

.
 When deriving the cross entropy, the term 
\begin_inset Formula $H(Y)$
\end_inset

 is ignored as it is assumed constant, and hence does not effect optimization,
 while the conditional entropy 
\begin_inset Formula $H(Y|Z)$
\end_inset

 is developed into the 
\begin_inset Formula $CE$
\end_inset

 term.
 We derive a new upper bound for the IB objective by not omitting 
\begin_inset Formula $H(Y)$
\end_inset

 from the distortion term.
 Subsequently, the variational approximation for our proposed bound is a
 variationaly tighter when taking into account that the optimization process
 is done over 
\begin_inset Formula $P(\hat{Y}|Z)$
\end_inset

 as well as 
\begin_inset Formula $P(Z|X)$
\end_inset

.
 This modification attains a tighter variational bound on the IB objective
 for any 
\begin_inset Formula $Y$
\end_inset

 with positive entropy, and a tighter empirical bound for all 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Subsection
IB upper bound
\end_layout

\begin_layout Standard
We begin by establishing a new upper bound for the IB functional by bounding
 the mutual information terms, using the same method as in VIB.
\end_layout

\begin_layout Standard

\bar under
Consider 
\begin_inset Formula $I(Z;X)$
\end_inset


\bar default
:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;X)=\int\int p^{*}(x,z)log\left(p^{*}(z|x)\right)dxdz- & \int p^{*}(z)log\left(p^{*}(z)\right)dz\label{eq:i_z_x1}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $r$
\end_inset

 we have that 
\begin_inset Formula $D_{KL}\left(p^{*}(z)\big|\big|r(z)\right)\ge0$
\end_inset

, it follows that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\int p^{*}(z)log\left(p^{*}(z)\right)dz\ge\int p^{*}(z)log\left(r(z)\right)dz\label{eq:d_kl}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And so, by Equation
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:d_kl}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
I(Z;X)\le\int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\label{eq:i_z_x2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard

\bar under
Consider 
\begin_inset Formula $I(Z;Y)$
\end_inset


\bar default
:
\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $c$
\end_inset

 we have that 
\begin_inset Formula $D_{KL}\left(p^{*}(y|z)\big|\big|c(y|z)\right)\ge0$
\end_inset

, it follows that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\int p^{*}(y|z)log\left(p^{*}(y|z)\right)dy\ge\int p^{*}(y|z)log\left(c(y|z)\right)dy\label{eq:d_kl2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And so, by Equation
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:d_kl2}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)= & \int\int p^{*}(y,z)log\left(\frac{p^{*}(y,z)}{p^{*}(y)p^{*}(z)}\right)dydz\ge\int\int p^{*}(y|z)p^{*}(z)log\left(\frac{c(y|z)}{p^{*}(y)}\right)dydz\nonumber \\
= & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz+H_{p^{*}}(Y)\label{eq:i_z_y}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We now diverge from the original VIB derivation by replacing 
\begin_inset Formula $H_{p^{*}}(Y)$
\end_inset

 with 
\begin_inset Formula $H_{c}(Y|Z)$
\end_inset

 instead of omitting it.
 In addition, we limit the new term to make sure that the inequality holds.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)\ge\int\int p^{*}(y,z)log\left(c(y|z)\right)dydz+min\left\{ H_{p^{*}}(Y),H_{c}(Y|Z)\right\} \label{eq:i_z_y2}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We further develop this term using the IB Markov chain 
\begin_inset Formula $Z\leftarrow X\leftarrow Y$
\end_inset

 and total probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)\ge & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz\nonumber \\
+ & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \label{eq:i_z_y3}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Finally, we join Equation 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_x2}
\end_layout

\end_inset

 with Equation 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_y3}
\end_layout

\end_inset

 to establish a new upper bound for the IB objective:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
L_{IB}\le L_{UB}\equiv & \int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\nonumber \\
-\int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz- & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \label{eq:l_ub}
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Variational approximation
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $e(z|x)$
\end_inset

 a variational encoder approximating 
\begin_inset Formula $p^{*}(z|x)$
\end_inset

 and 
\begin_inset Formula $c(y|z)$
\end_inset

 a variational classifier approximating 
\begin_inset Formula $p^{*}(y|z)$
\end_inset

.
 We define the variational approximation 
\begin_inset Formula $L_{VUB}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
L_{UB}\approx L_{VUB}\equiv & \beta\int\int p^{*}(x)e(z|x)log\left(\frac{e(z|x)}{r(z)}\right)dxdz\nonumber \\
- & \int\int\int p^{*}(x)p^{*}(y|x)e(z|x)log\left(c(y|z)\right)dxdydz\label{eq:l_vub}\\
- & min\Bigg\{ H_{p^{*}}(Y),-\int\int\int p^{*}(x)e(z|x)c(y|z)log\left(c(y|z)\right)dxdydz\Bigg\}\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Empirical estimation
\end_layout

\begin_layout Standard
The real and continuous distribution 
\begin_inset Formula $p^{*}(x,y)=p^{*}(y|x)p^{*}(x)$
\end_inset

 can be estimated by Monte Carlo sampling from a discrete dataset 
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none

\begin_inset Formula $\mathcal{S}$
\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
, and distributions featuring 
\begin_inset Formula $Z$
\end_inset

 are sampled from a stochastic encoder.
 Let 
\begin_inset Formula $e_{\phi}(z|x)\sim N(\mu,\Sigma)$
\end_inset

 be a stochastic DNN encoder with parameters 
\begin_inset Formula $\phi$
\end_inset

, and a final layer of dimension 
\begin_inset Formula $2K$
\end_inset

, such that for each forward pass, the first 
\begin_inset Formula $K$
\end_inset

 entries are used to encode 
\begin_inset Formula $\mu$
\end_inset

, and the last 
\begin_inset Formula $K$
\end_inset

 entries to encode a diagonal 
\begin_inset Formula $\Sigma$
\end_inset

, after a soft-plus transformation.
 For each 
\begin_inset Formula $x_{n}\in\mathcal{S}$
\end_inset

 we generate a sample 
\begin_inset Formula $\hat{z}_{n}$
\end_inset

 from the encoder, using the reparameterization trick 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

.

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 Let 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $C_{\lambda}$
\end_inset

 be a discrete classifier neural net parameterized by 
\begin_inset Formula $\lambda$
\end_inset

, such that 
\begin_inset Formula $C_{\lambda}(y|z)\sim Categorical$
\end_inset

, let 
\begin_inset Formula $H_{\mathcal{S}}(Y)$
\end_inset

 be the empirical entropy of the real RV 
\begin_inset Formula $Y$
\end_inset

 as measured from the training data, and let 
\begin_inset Formula $\hat{Y}$
\end_inset

 be the variational approximation for 
\begin_inset Formula $Y$
\end_inset

.
 We chose a standard Gaussian as a variational approximation for the marginal
 
\begin_inset Formula $r(z)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{L}_{VUB}\equiv\frac{1}{N}\sum_{n=1}^{N}\left[\beta D_{KL}\left(e_{\phi}(z|x_{n})\Bigg|\Bigg|r(z)\right)-log\left(C_{\lambda}\left(y_{n}\big|\hat{z}_{n}\right)\right)-min\left\{ H_{\mathcal{S}}(Y),H_{C_{\lambda}}\left(\hat{Y}|Z\right)\right\} \right]\label{eq:l_vub_empirical}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
Interpretation
\end_layout

\begin_layout Standard
The conditional entropy term that follows from our proposed derivation provides
 strong classifier regularization, as shown in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

.
 It was also shown that the quality of representations learned by ELBO loss
 functions degenerates as a consequence of overfitting decoders 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

.
 As VIB is in fact a beta regulated ELBO loss, it follows that regulating
 its classifier can lead to better representations learned.
 We propose an additional theoretical motivation for the effects of entropy
 regularization on supervised learning: Following the variational IB Markov
 chain 
\begin_inset Formula $Y\leftrightarrow X\text{\ensuremath{\rightarrow}}Z\rightarrow\hat{Y}$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Geiger2020}
\end_layout

\end_inset

 we have that 
\begin_inset Formula $\hat{Y}$
\end_inset

 is a deterministic function of 
\begin_inset Formula $Z$
\end_inset

, and so 
\begin_inset Formula $H(Z)\ge H(\hat{Y})$
\end_inset

 by the data processing inequality 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Cover1999elements}
\end_layout

\end_inset

.
 The following inequality holds:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
2H(\hat{Y})-H(\hat{Y},Z)\overset{\text{DPI}}{\le}H(\hat{Y})+H(Z)-H(\hat{Y},Z)=I(Z;\hat{Y})\overset{(\ref{eq:i_z_y})}{\le}I(Z;Y)\le min\left(H(Z),H(Y)\right)
\]

\end_inset


\end_layout

\begin_layout Standard
For the left most expression, we have that any increase in 
\begin_inset Formula $H(\hat{Y})$
\end_inset

 will increase the joint entropy 
\begin_inset Formula $H(\hat{Y},Z)$
\end_inset

 by at most the same magnitude, leading to the proportionality 
\begin_inset Formula $H(\hat{Y})\propto2H(\hat{Y})-H(\hat{Y},Z)$
\end_inset

.
 It follows that the true mutual information 
\begin_inset Formula $I(Z;Y)$
\end_inset

 is squeezed between 
\begin_inset Formula $min\left(H(Z),H(Y)\right)$
\end_inset

 and 
\begin_inset Formula $H(\hat{Y})\ge H(\hat{Y}|Z)$
\end_inset

.
 We propose that the increase in variational entropy, that follows from
 VUB, can skew the true mutual information 
\begin_inset Formula $I(Z;Y)$
\end_inset

 up.
 Since the real 
\begin_inset Formula $Y$
\end_inset

 is constant, this increase can only be caused by learning a better representati
on 
\begin_inset Formula $Z$
\end_inset

.
 Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{venn_diagram}
\end_layout

\end_inset

 illustrates the possible effect of an increase in variational entropy:
 The left hand diagram suggest a model with low variational entropy, and
 a low variational mutual information, and the right hand diagram suggest
 a model with increased variational entropy.
 The cross entropy term pushes 
\begin_inset Formula $I(Z;\hat{Y})$
\end_inset

 up, and the real mutual information 
\begin_inset Formula $I(Z;Y)$
\end_inset

 increases as a result of the Barber-Agakov inequality 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Barber2003}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/vann_diagram_horizonthal.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Venn diagrams illustrating the possible increase in real mutual information
 as a result of increased variational entropy.
 The right diagram features higher variational entropy, and higher variational
 mutual information that induce higher real mutual information as a result
 of the Barber-Agakov inequality 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Barber2003}
\end_layout

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "venn_diagram"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Experiments
\begin_inset CommandInset label
LatexCommand label
name "sec:Experiments"

\end_inset


\end_layout

\begin_layout Standard
We follow the experimental setup proposed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

, extending it to NLP tasks as well.
 We trained image classification models on the ImageNet 2012 dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{deng2009imagenet}
\end_layout

\end_inset

, and text classification on
\series bold
 
\series default
the IMDB
\series bold
 
\series default
sentiment analysis dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{maas2011imdb}
\end_layout

\end_inset

.
 For each dataset, we compared a competitive Vanilla model with a VIB and
 a VUB model trained with beta values of 
\begin_inset Formula $\beta=10^{-i}$
\end_inset

 for 
\begin_inset Formula $i\in\{1,2,3\}$
\end_inset

.
 Each model was trained and evaluated 5 times per 
\begin_inset Formula $\beta$
\end_inset

 value, with consistent performance and statistical significance shown by
 a Wilcoxon rank sum test.
 Each model was evaluated using test set accuracy, and robustness to various
 adversarial attacks.
 For image classification, we employed the untargeted Fast Gradient Sign
 (FGS) attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Goodfellow2015}
\end_layout

\end_inset

 as well as the targeted CW 
\begin_inset Formula $L_{2}$
\end_inset

 attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Carlini2017}, 
\backslash
cite{Kaiwen2018}
\end_layout

\end_inset

.
 For text classification, we used the untargeted Deep Word Bug attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{gao2018deepwordbug}, 
\backslash
cite{Morris2020}
\end_layout

\end_inset

 as well as the untargeted PWWS attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ren2019pwws}
\end_layout

\end_inset

.
 Elaboration on the experimental setup, results and further insights from
 the experiments are available in Appendix
\begin_inset space ~
\end_inset

C.
 Code to reconstruct the experiments is provided to the supplementary materials
 of this paper.
\end_layout

\begin_layout Subsection
Image classification
\end_layout

\begin_layout Standard
A pre-trained inceptionV3 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 base model was used and achieved a 77.21% accuracy on the ImageNet 2012
 validation set (Test set for ImageNet is unavailable).
 Image classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figures
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:untargeted_examples}, 
\backslash
ref{fig:targeted_examples}
\end_layout

\end_inset

 in Appendix
\begin_inset space ~
\end_inset

C.
 The empirical results presented in Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

 confirm that while VIB reduces performance on the validation set, it substantia
lly improves robustness to adversarial attacks.
 Moreover, these results demonstrate that VUB significantly outperforms
 VIB in terms of validation accuracy, while providing competitive robustness
 to attacks similarly to VIB.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="5">
<features booktabs="true" tabularvalignment="middle" tabularwidth="12cm">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Val 
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.1}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.5}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
CW
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
77.2%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
68.9%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
67.7%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
788
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
73.7%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
59.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
63.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3917
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm291$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
72.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
53.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
72.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.01\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
58.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.03\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
62.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
66.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2666
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm140$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.05\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
57.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64.3%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1564
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm218$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
74.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.09\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
57.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
64.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3575
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm456$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ImageNet evaluation scores for vanilla, VIB and VUB models, average over
 5 runs with standard deviation.
 First column is performance on the ImageNet validation set (higher is better
 
\begin_inset Formula $\uparrow$
\end_inset

), second and third columns are the % of successful FGS attacks at 
\begin_inset Formula $\epsilon=0.1,0.5$
\end_inset

 (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

), and the fourth column is the average 
\begin_inset Formula $L_{2}$
\end_inset

 distance for a successful Carlini Wagner 
\begin_inset Formula $L_{2}$
\end_inset

 targeted attack (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

).
 VUB attains significantly higher accuracy over unseen data in all settings,
 while preserving competitive robustness to adversarial attacks.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:imagenet_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Text classification
\begin_inset CommandInset label
LatexCommand label
name "sec:text_classification"

\end_inset


\end_layout

\begin_layout Standard
A fine tuned BERT uncased 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Devlin2019}
\end_layout

\end_inset

 base model was used and achieved a 93.0% accuracy on the
\series bold
 
\series default
IMDB sentiment analysis test set.
 Text classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_examples}
\end_layout

\end_inset

 in Appendix
\begin_inset space ~
\end_inset

C.
 In this modality, VUB significantly outperforms VIB in both test set accuracy
 and robustness to the two attacks.
 Moreover, VUB also outperformed the original model in terms of test set
 accuracy.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="4">
<features booktabs="true" tabularvalignment="middle" tabularwidth="12cm">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Test
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
DWB
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
PWWS
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
93.0%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
54.3%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
100%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
91.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
35.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.4\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
41.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm6.6\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
90.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
41.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
62.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm14.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
89.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.9\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
90.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm8.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
99.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.9\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
93.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
27.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
28.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
92.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
30.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
50.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
89.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
99.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
100%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
IMDB evaluation scores for vanilla, VIB and VUB models, average over 5 runs
 with standard deviation.
 First column is performance over the test set (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

), second is % of successful Deep Word Bug attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

) and third is % of successful PWWS attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

).
 In almost all cases VUB attains significantly higher accuracy over unseen
 data, as well as significantly higher robustness to adversarial attacks.
 For this modality, VUB also outperforms the vanilla model in terms of test
 set accuracy for 
\begin_inset Formula $\beta=10^{-3}$
\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
The IB is a private case of rate distortion, and is designed to learn representa
tions in unsupervised settings.
 Adapting the IB to supervised settings requires maximization of mutual
 information between compressed representation and learned discriminator,
 rather than the real and unknown downstream RV.
 This distinction requires lifting the assumption that the downstream distributi
on is known during the derivation of a tractable objective.
 Lifting this assumption allows us to derive a tighter variational bound
 on the IB, which we show to produce better classification accuracy, with
 equivalent or superior robustness to adversarial attacks, over high dimensional
 tasks of different modalities.
 We extend previous theoretical studies on representation learning 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

 to provide a possible insight to the effects of our proposed derivation
 on the quality of the learned representation.
 We show that, apart from the regularization described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyara2017}
\end_layout

\end_inset

, increasing classifier entropy enables the variational mutual information
 between representation and classifier to better approximate the real mutual
 information between representation and the real underlying distribution,
 bringing the attained rate distortion ratio closer to its theoretical IB
 limit.
 While other advancements have been done in recent years, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Fischer2020,Cheng2020}
\end_layout

\end_inset

, none show a tighter bound than VIB, and all modify the derivation of the
 rate term, while our work both derives an upper bound by modifying the
 distortion term.
\end_layout

\begin_layout Standard
While providing a complete framework for optimal data modeling, the IB,
 and its variational approximations, rely on three assumptions: (1) It suffices
 to optimize the mutual information metric to optimize a model’s performance;
 (2) Forgetting more information about the input, while keeping relevant
 information about the output, induces better generalization; (3) Mutual
 information between the input, output and latent representation can be
 either computed or approximated to a desired level of accuracy.
 Our improved empirical results, induced by a tighter bound, suggest better
 data modeling, and strengthen the cause for the IB, and its variational
 approximations, as a learning framework for classifier DNNs.
 Conversely, one might argue that the improved adversarial robustness of
 both VIB and VUB is an artifact of latent geometry: Both methods promote
 a disentangled latent space by using a stochastic factorized prior, as
 suggested by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Chen2018}
\end_layout

\end_inset

.
 In addition, both utilize KL regularization, enforcing clustering around
 a 
\begin_inset Formula $0$
\end_inset

 mean, which might increase latent smoothness.
 These geometric traits of the latent space can make it difficult for minor
 perturbations to significantly alter latent semantics, possibly making
 the models more robust to attacks.
 
\end_layout

\begin_layout Standard
In conclusion, while the IB and its variational approximations do not provide
 a complete theoretical framework for DNN data modeling and regularization,
 they offer a strong, measurable, and theoretically grounded approach.
 VUB is presented as a tractable and tighter upper bound of the IB functional,
 that can be easily adapted to any classifier DNN to significantly increase
 robustness to various adversarial attacks, while inflicting minimal decrease
 in test set performance, and in some cases even increasing it.
 
\end_layout

\begin_layout Standard
This study opens many opportunities for further research: Further improvements
 to the upper bound, including combining VUB with proposed new bounds for
 the IB rate term such as CLUB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Cheng2020}
\end_layout

\end_inset

; Applying VUB in self-supervised learning, and in particular to measure
 whether representations learned with VUB capture better semantics than
 representations learned with non IB inspired loss functions; Finally, comparing
 the effects of latent geometry vs rate distortion on adversarial robustness
 is left to future work.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bibliography{draft.bib}
\end_layout

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
setcounter{section}{1}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Section*
\start_of_appendix
Appendix A - Preliminaries
\begin_inset CommandInset label
LatexCommand label
name "sec:AppendixA"

\end_inset


\end_layout

\begin_layout Standard
We denote random variables (RVs) with upper cased letters 
\begin_inset Formula $X,Y$
\end_inset

, and their realizations in lower case 
\begin_inset Formula $x,y$
\end_inset

.
 Denote discrete Probability Mass Functions (PMFs) with an upper case 
\begin_inset Formula $P(x)$
\end_inset

 and continuous Probability Density Functions (PDFs) with a lower case 
\begin_inset Formula $p(x)$
\end_inset

.
 Hat notation denotes empirical measurements.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X,Y$
\end_inset

 be two observed random variables with an unknown joint distribution 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

 and marginals 
\begin_inset Formula $p^{*}(x),\,p^{*}(y)$
\end_inset

.
 We can attempt to approximate these distributions using a model 
\begin_inset Formula $p_{\theta}$
\end_inset

 with parameters 
\begin_inset Formula $\theta$
\end_inset

, such that for generative tasks 
\begin_inset Formula $p_{\theta}(x)\approx p^{*}(x)$
\end_inset

, and for discriminative tasks 
\begin_inset Formula $p_{\theta}(y|x)\approx p^{*}(y|x)$
\end_inset

, using a dataset 
\begin_inset Formula $\mathcal{S}=\left\{ (x_{1},y_{1}),...,(x_{N},y_{N})\right\} $
\end_inset

 to fit our model.
 One can also assume the existence of an additional unobserved RV 
\begin_inset Formula $Z\sim p^{*}(z)$
\end_inset

 that influences or generates the observed RVs 
\begin_inset Formula $X,Y$
\end_inset

.
 Since 
\begin_inset Formula $Z$
\end_inset

 is unobserved, it is absent from the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

, and so cannot be modeled directly.
 Denote 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x|z)p_{\theta}(z)dz=\int p_{\theta}(x,z)dz$
\end_inset

 the marginal, 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

 the prior as it is not conditioned over any other RV, and 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

 the posterior following Bayes' rule.
\end_layout

\begin_layout Standard
When modeling an unobserved variable of an unknown distribution, we encounter
 a problem as the marginal 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x,z)dz$
\end_inset

 doesn't have an analytic solution.
 This intractability can be overcome by choosing some tractable parametric
 variational distribution 
\begin_inset Formula $q_{\phi}(z|x)$
\end_inset

 to approximate the posterior 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

, such that 
\begin_inset Formula $q_{\phi}(z|x)\approx p_{\theta}(z|x)$
\end_inset

, and estimate 
\begin_inset Formula $p_{\theta}(x,z)$
\end_inset

 or 
\begin_inset Formula $p_{\theta}(x,z|y)$
\end_inset

 by fitting the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2019}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Vapnik1995}
\end_layout

\end_inset

 defines 
\emph on
supervised
\emph default
 learning as follows:
\end_layout

\begin_layout Itemize
A generator of random vectors 
\begin_inset Formula $x\in\mathbb{R}^{d}$
\end_inset

, drawn independently from an unknown probability distribution 
\begin_inset Formula $p^{*}(x)$
\end_inset

.
\end_layout

\begin_layout Itemize
A supervisor who returns a scalar output value 
\begin_inset Formula $y\in\mathbb{R}$
\end_inset

, according to an unknown conditional probability distribution 
\begin_inset Formula $p^{*}(y|x)$
\end_inset

.
 We note that these probabilities can indeed be soft labels, where 
\begin_inset Formula $y$
\end_inset

 is a continuous probability vector, rather the more commonly used hard
 labels.
\end_layout

\begin_layout Itemize
A learning machine capable of implementing a predefined set of functions,
 
\begin_inset Formula $f(x,\theta):\mathbb{R}^{d}\times\Theta\mapsto\mathbb{R}$
\end_inset

, where 
\begin_inset Formula $\Theta$
\end_inset

 is a set of parameters.
\end_layout

\begin_layout Standard
The problem of learning is that of choosing from the given set of functions,
 the one that best approximates the supervisor’s response.
 The choice is typically based on a training set of 
\begin_inset Formula $n$
\end_inset

 independent and identically distributed pairs of observations drawn according
 to 
\begin_inset Formula $p(x,y)=p(x)p(y|x):\mathcal{S}$
\end_inset

.
 
\end_layout

\begin_layout Standard
Given a set of unlabeled data points 
\begin_inset Formula $\{x_{1},...,x_{n}\},x_{i}\in\mathbb{R}^{d}$
\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Slonim2002}
\end_layout

\end_inset

 defines 
\emph on
unsupervised
\emph default
 learning as the task of constructing a compact representation of these
 points, which in some sense reveals their hidden structure.
 This representation can be used further to achieve a variety of goals,
 including reasoning, prediction, communication etc.
 In particular, unsupervised clustering partitions the data points into
 exhaustive and mutually exclusive clusters, where each cluster can typically
 be represented by a centroid, typically a weighted average of the cluster's
 members.
 Soft clustering assigns cluster probabilities for each data point, and
 fits an assignment by minimizing the expected loss for these probabilities,
 usually a distance metric such as MSE.
\end_layout

\begin_layout Standard
In this work, information theoretic functions share the same notation for
 discrete and continuous settings and are denoted as follows:
\end_layout

\begin_layout Standard
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="4">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="middle" width="2cm">
<column alignment="left" valignment="middle" width="2.2cm">
<column alignment="left" valignment="middle" width="4.1cm">
<column alignment="left" valignment="middle" width="4.1cm">
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Notation
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\series bold
Differential
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\align center

\series bold
Discrete
\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $H_{p}(X)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $-\int p(x)log\left(p(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\sum_{x\in X}P(x)log\left(P(x)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Conditional
\end_layout

\begin_layout Plain Layout

\series bold
entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $H_{p}(X|Y)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="middle" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\int\int p(x,y)log\left(p(x|y)\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $dxdy$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\sum_{x\in X}\sum_{y\in Y}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $P(x,y)log\left(P(x|y)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Cross
\end_layout

\begin_layout Plain Layout

\series bold
entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $CE(p,q)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\int p(x)log\left(q(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\sum_{x\in X}P(x)log\left(Q(x)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Joint
\end_layout

\begin_layout Plain Layout

\series bold
entropy
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $H_{p}(X,Y)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\int\int p(x,y)log\left(p(x,y)\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $dxdy$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $-\sum_{x\in X}\sum_{y\in Y}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $P(x,y)log\left(P(x,y)\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
KL
\end_layout

\begin_layout Plain Layout

\series bold
divergence
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $D_{KL}\left(p\big|\big|q\right)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\int p(x)log\left(\frac{p(x)}{q(x)}\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sum_{x\in X}P(x)log\left(\frac{P(x)}{Q(x)}\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Mutual
\end_layout

\begin_layout Plain Layout

\series bold
information (MI)
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $I(X;Y)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\int\int p(x,y)log\left(\frac{p(x,y)}{p(x)p(y)}\right)$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $dxdy$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sum_{x\in X}\sum_{y\in Y}$
\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Formula $P(x,y)log\left(\frac{P(x,y)}{P(x)P(y)}\right)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Section*
\start_of_appendix
Appendix B - Related work elaboration
\begin_inset CommandInset label
LatexCommand label
name "sec:AppendixB"

\end_inset


\end_layout

\begin_layout Standard
This appendix expands the related work Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:related_work}
\end_layout

\end_inset

, by providing a deeper review of the IB, the IB theory of deep learning,
 and variational approximations for the IB.
\end_layout

\begin_layout Subsection*
The Information Plane
\end_layout

\begin_layout Standard
As mentioned in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:deterministic_ib}
\end_layout

\end_inset

, the solution to the IB objective, 
\begin_inset Formula $\mathcal{L}_{IB}=I(X;Z)-\beta I(Z;Y)$
\end_inset

, depends on the Lagrange multiplier 
\begin_inset Formula $\beta$
\end_inset

.
 Hence, the IB objective has no one unique solution, and can thus be plotted
 as a function of 
\begin_inset Formula $\beta$
\end_inset

 and of 
\begin_inset Formula $Z$
\end_inset

's cardinality, over a Cartesian system composed of the axes 
\begin_inset Formula $I(X;Z)$
\end_inset

 (rate) and 
\begin_inset Formula $I(Z;Y)$
\end_inset

 (distortion).
 When plotted as a function of 
\begin_inset Formula $\beta$
\end_inset

 the IB functional is called the 'information curve' and its Cartesian system
 the 'information plane' 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

, as illustrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

.
 When 
\begin_inset Formula $\beta$
\end_inset

 approaches 
\begin_inset Formula $0$
\end_inset

 the distortion term is nullified and we learn a representation that has
 no information over the down stream task, and maximal compression (such
 a representation may be a null vector).
 When 
\begin_inset Formula $\beta$
\end_inset

 approaches 
\begin_inset Formula $\infty$
\end_inset

 we learn a representation that has the maximal possible information over
 the downstream task, but that also holds the maximal information over the
 training data, hence overfitted.
 The region above the information curve is unreachable by any possible represent
ation.
 The different bifurcation of the information curve, illustrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

, correspond to the different possible cardinalities of the compressed represent
ation.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/info_plane.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The information plane and curve: rate-distortion ratio over 
\begin_inset Formula $\beta$
\end_inset

.
 At 
\begin_inset Formula $\beta=0$
\end_inset

 the representation is compressed but uninformative (maximal compression),
 at 
\begin_inset Formula $\beta\rightarrow\infty$
\end_inset

 the representation is informative but potentially overfitted (maximal informati
on).
 Taken from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Slonim2002}
\end_layout

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Fixing a Broken Elbo
\end_layout

\begin_layout Standard
As mentioned in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:it_regularization}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 extends the information plane with an additional theoretical bound for
 rate distortion ratio, imposed by the usage of finite parametric families
 of variational approximations.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 observed that the KL regularization term in the ELBO loss function isn't
 directly affected by the quality of the reconstructed image, and vice versa.
 Following this logic, given a powerful enough decoder, optimizing the ELBO
 loss can result in poor latent representation, compensated for by an overfitted
 decoder.
 Mutual information is used to measure representation quality: 
\begin_inset Formula $I_{e}(X;Z)\equiv\int\int p_{e}(x,z)log\left(\frac{p_{e}(x,z)}{p^{*}(x)p_{e}(z)}\right)dxdz$
\end_inset

, for a stochastic encoder 
\begin_inset Formula $e$
\end_inset

.
 
\begin_inset Formula $I_{e}$
\end_inset

 can be bounded from both sides as follows: 
\begin_inset Formula $H_{p^{*}}(X)-I_{d}(X;\hat{X})\le I_{e}(X;Z)\le D_{KL}\left(q_{\phi}(z|x)\big|\big|p_{\theta}(z)\right)$
\end_inset

, where 
\begin_inset Formula $H_{p^{*}}(X)$
\end_inset

 is the true data entropy, 
\begin_inset Formula $I_{d}$
\end_inset

 is the variational decoder's distortion.
 It is observed, that the KL term does not depend on the variational decoder
 distribution, and can be minimized regardless of reconstruction quality.
 Equivalently, good reconstruction does not directly depend on good representati
on.
 Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{phase_diagram}
\end_layout

\end_inset

 is suggested as an extension to the information plane 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

, by replacing real rate and distortion with their variational approximations.
 The new plane is divided into three sub planes: (1) Infeasible: This is
 the IB theoretical limit (As per Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

); (2) Feasible: Attainable given an infinite model family, and complete
 variety of: 
\begin_inset Formula $e(z|x),d(x|z)$
\end_inset

 and 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

; (3) Realizable: Attainable given a finite parametric and tractable variational
 family.
\end_layout

\begin_layout Standard
The black diagonal line at the lower left of Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{phase_diagram}
\end_layout

\end_inset

 satisfies 
\begin_inset Formula $H_{p^{*}}(X)-I_{d}(X;\hat{X})=D_{KL}\left(q_{\phi}(z|x)\big|\big|p_{\theta}(z)\right)$
\end_inset

, resulting in tight variational bounds on the mutual information: 
\begin_inset Formula $R\le I_{e}\le R$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

 notes that a common degeneration of representations in VAEs is a decrease
 in rate and distortion over the training data, together with an increase
 in both over unseen data.
 This is caused by overpowerful decoders that overfit an uninformative learned
 representation.
 Low ELBO loss can be attained as both the distortion and rate terms are
 minimized during training.
 
\begin_inset Formula $D_{KL}\left(q_{\phi}(z|x)\big|\big|p_{\theta}(z)\right)$
\end_inset

 approaches 
\begin_inset Formula $0$
\end_inset

 iff 
\begin_inset Formula $e(z|x)\rightarrow p_{\theta}(z)$
\end_inset

.
 In this case, 
\begin_inset Formula $e(z|x)$
\end_inset

 is close to independence of 
\begin_inset Formula $x$
\end_inset

, and the latent representation fails to encode information about the input.
 However, a suitably powerful decoder could non the less learn to overfit
 encoded traces of the training examples, and reach a low distortion score
 during optimization.
 In the current study, we extend this theoretical framework to explain the
 advancements of our proposed loss function.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/extended_info_plane.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Phase diagram, a proposed information plane interpretation of VAEs.
 Axes are variational rate and distortion.
 The IB theoretical limit is extended by an additional limit induced by
 the constraint of a finite parametric variational family.
 Once a family is chosen, we seek to learn an optimal marginal 
\begin_inset Formula $m(z)$
\end_inset

 and decoder 
\begin_inset Formula $d(x|z)$
\end_inset

 in order to approach the new limit.
 
\begin_inset Formula $\beta$
\end_inset

 modulation controls the tradeoff between rate and distortion, regardless
 of the variational family.
 Note that this figure is inverted in orientation to Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

, i.e.
 low distortion corresponds to better performance, and not to lower MI.
 Taken from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "phase_diagram"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
IB theory of deep learning
\end_layout

\begin_layout Standard
The following is a summary of work leveraging the IB framework for deterministic
 DNN optimization and interpretation.
 For a more comprehensive review of this opinion-splitting topic, the reader
 is advised to consult the work of 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Goldfeld2020}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Tishby2015}
\end_layout

\end_inset

 proposed a representation-learning interpretation of DNNs using the IB
 framework, regarding DNNs as Markov cascades of intermediate representations
 between hidden layers.
 Under this notion, comparing the optimal and the achieved rate-distortion
 ratios between DNN layers will indicate if a model is too complex or too
 simple for a given task and training set.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

 visualized and analyzed the information plane behavior of DNNs over a toy
 problem with a known joint distribution.
 Mutual information of the different layers was estimated and used to analyze
 the training process.
 The learning process over Stochastic Gradient Descent (SGD) exhibited two
 separate and sequential behaviors: A short Empirical Error Minimization
 phase (ERM) characterized by a rapid decrease in distortion, followed by
 a long compression phase with an increase in rate until convergence to
 an optimal IB limit as demonstrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{shwartz_ziv_info_plane}
\end_layout

\end_inset

.
 Similar yet repetitive behavior was observed in the current study, as elaborate
d in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{subsec:Evaluation-and-analysis}
\end_layout

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../media/black_box_z.png
	scale 36

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Information plane scatters of different DNN layers (colors) in 50 randomized
 networks.
 From 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 Left are initial weights, Right are at 400 epochs.
 Our study reproduced similar yet repetitive behavior on complicated high
 dimensional tasks, as elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{subsec:Evaluation-and-analysis}
\end_layout

\end_inset

 and in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "shwartz_ziv_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Saxe2018}
\end_layout

\end_inset

 reproduced the experiments described in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ShwartzZiv2017}
\end_layout

\end_inset

, expanding them to different activation functions, different datasets and
 different methods to estimate mutual information.
 It was found that double-sided saturated nonlinear activations, such as
 the tanh, produced a distinct compressions stage when mutual information
 was measured by binning, as performed in 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ShwartzZiv2017}
\end_layout

\end_inset

, while other activations did not.
 It was also shown that DNN generalization did not depend on a distinct
 compression stage, and that DNNs do forget task irrelevant information,
 but this happens concurrently to the learning of task relevant information,
 and not necessarily in a distinct phase.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Amjad2020}
\end_layout

\end_inset

 also showed that equivalent representations might yield the same IB loss
 while one achieved better classification rate than the other.
 These discrepancies are caused because mutual information in deterministic
 DNNs is either infinite or step like, because of mutual information's invarianc
e to invertible transformations and because of the absence of a decision
 function in the objective.
\end_layout

\begin_layout Standard
When examining the information plane behavior in the current study, we notice
 recurring patterns of distortion reduction followed by rate increase, resemblin
g the ERM and representation compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

, as elaborated in Appendix
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:AppendixC}
\end_layout

\end_inset

.

\series bold
 
\end_layout

\begin_layout Subsection*
Conditional Entropy Bottleneck
\end_layout

\begin_layout Standard
As mentioned in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{subsec:variational_approximations}
\end_layout

\end_inset

, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Fischer2020}
\end_layout

\end_inset

 showed that the conditional entropy bottleneck is equivalent to IB for
 
\begin_inset Formula $\gamma=\beta-1$
\end_inset

 following the chain rule of mutual information and the IB Markov chain.
 We develop this equivalence in detail:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
CEB= & I(X;Z|Y)-\gamma I(Z;Y)\\
\overset{\text{MI chain rule}}{=} & H(Z|Y)-H(Z|X,Y)-\gamma I(Z;Y)\\
\overset{Z\leftarrow X\leftrightarrow Y}{=} & H(Z|Y)-H(Z|X)-\gamma I(Z;Y)\\
\overset{\gamma:=\beta-1}{\Longrightarrow} & H(Z|Y)-H(Z|X)-(\beta-1)I(Z;Y)\\
= & H(Z|Y)-H(Z|X)-\beta I(Z;Y)+I(Z;Y)\\
= & H(Z|Y)-H(Z|X)-\beta I(Z;Y)+H(Z)-H(Z|Y)\\
= & H(Z)-H(Z|X)+H(Z|Y)-H(Z|Y)-\beta I(Z;Y)\\
= & I(X;Z)-\beta I(Z;Y)
\end{align*}

\end_inset


\end_layout

\begin_layout Section*
\start_of_appendix
Appendix C - Experiments elaboration
\begin_inset CommandInset label
LatexCommand label
name "sec:AppendixC"

\end_inset


\end_layout

\begin_layout Standard
Image classification models were trained on the first 500,000 samples of
 the ImageNet 2012 dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{deng2009imagenet}
\end_layout

\end_inset

, and text classification over
\series bold
 
\series default
the entire IMDB
\series bold
 
\series default
sentiment analysis dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{maas2011imdb}
\end_layout

\end_inset

.
 For each dataset, a competitive pre-trained model (Vanilla model) was evaluated
 and then used to encode embeddings.
 These embeddings were then used as a dataset for a new stochastic classifier
 net with either a VIB or a VUB loss function.
 Stochastic classifiers consisted of two ReLU activated linear layers of
 the same dimensions as the pre-trained model's logits (2048 for image and
 768 for text classification), followed by reparameterization and a final
 softmax activated FC layer.
 Learning rate was 
\begin_inset Formula $10^{-4}$
\end_inset

 and decaying exponentially with a factor of 0.97 every two epochs.
 Batch sizes were 32 for ImageNet and 16 for IMDB.
 All models were trained using an Nvidia RTX3080 GPU with approximately
 1-2 days per a single experiment run.
 Beta values of 
\begin_inset Formula $\beta=10^{-i}$
\end_inset

 for 
\begin_inset Formula $i\in\{1,2,3\}$
\end_inset

 were tested, and we used a single forward pass per sample for inference,
 since previous studies indicated that these are the best range and sample
 rate for VIB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Alemi2018}
\end_layout

\end_inset

.
 Each model was trained and evaluated 5 times per 
\begin_inset Formula $\beta$
\end_inset

 value, with consistent performance.
 Statistical significance was demonstrated in all comparisons using the
 Wilcoxon rank sum test as follows: For each compared metric a sorted vector
 of results was prepared, where each entry featured the attained result
 in each of the 5 i.i.d.
 experiments per algorithm, and a boolean indicator value for the algorithm
 type.
 All metrics compared attained a p-value of less than 
\begin_inset Formula $0.05$
\end_inset

.
 For example:
\begin_inset Formula $r=\left((0.94,1)\ (0.935,1)\ (0.93,1)\ (0.93,1)\ (0.925,1)\ (0.92,0)\ (0.915,0)\ (0.915,0)\ (0.91,0)\ (0.89,0)\right)^{-1}$
\end_inset

 be a sorted vector of (test accuracy, algorithm) tuples, 1 being VUB, 0
 VIB.
 We computed the rank-sum as follows: 
\begin_inset Formula 
\[
\mu_{T}=\frac{5\cdot11}{2}=27.5,\ \sigma_{T}=\sqrt{\frac{5\cdot5\cdot11}{12}}≈4.78,\ Z(T)=\frac{15-27.5}{4.78}≈-2.61
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\Phi^{-1}(pval)=-2.61,\ pval=0.0045≤0.05
\]

\end_inset


\end_layout

\begin_layout Standard
In practice, these were computed with the Python Scipy library:
\end_layout

\begin_layout LyX-Code
import scipy.stats as stats
\end_layout

\begin_layout LyX-Code
vib_scores = [0.915, 0.915, 0.91, 0.92, 0.89] 
\end_layout

\begin_layout LyX-Code
vub_scores = [0.93, 0.935, 0.925, 0.93, 0.94] 
\end_layout

\begin_layout LyX-Code
pvalue = stats.ranksums(vub_scores, vib_scores, 'greater').pvalue
\end_layout

\begin_layout LyX-Code
assert pvalue < 0.05
\end_layout

\begin_layout Subsection*
Image classification
\end_layout

\begin_layout Standard
The ImageNet 2012 validation set was used for evaluation as the test set
 for ImageNet is unavailable.
 InceptionV3 yields a slightly worse single shot accuracy than inceptionV2
 (80.4%) when run in a single model and single crop setting, however we've
 used InceptionV3 over V2 for simplicity.
 Each model was trained for 100 epochs.
 The entire validation set was used to measure accuracy and robustness to
 FGS attacks, while only 1% of it was used for CW attacks, as they are computati
onally expensive.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/image_attacks/untargeted_attacks.png
	scale 26.5

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful untargeted FGS attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 Perturbation magnitude is determined by the parameter 
\begin_inset Formula $\epsilon$
\end_inset

 shown on the left, the higher, the more perturbed.
 Original and wrongly assigned labels are listed at the top of each image.
 Notice the deterioration of image quality as 
\begin_inset Formula $\epsilon$
\end_inset

 increases.
\begin_inset CommandInset label
LatexCommand label
name "fig:untargeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/image_attacks/targeted_attacks.png
	scale 45

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful targeted CW attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 The target label is 'Soccer ball'.
 Average 
\begin_inset Formula $L_{2}$
\end_inset

 distance required for a successful attack is shown on the left.
 The higher the required 
\begin_inset Formula $L_{2}$
\end_inset

 distance, the greater the visible change required to fool the model.
 Original and wrongly assigned labels are listed at the top of each image.
 Mind the difference in noticeable change as compared to the FGS perturbations
 presented in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:untargeted_examples}
\end_layout

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "fig:targeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Text classification
\begin_inset CommandInset label
LatexCommand label
name "sec:text_classification-1"

\end_inset


\end_layout

\begin_layout Standard
Each model was trained for 150 epochs.
 The entire test set was used to measure accuracy, while only the first
 200 entries in the test set were used for adversarial attacks as they are
 computationally expensive.
\end_layout

\begin_layout Standard
\begin_inset Float table
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="1">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="8cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Original text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
the acting , costumes , music , cinematography and sound are all 
\emph on
astounding
\emph default
 given the production 's austere locales .
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Perturbed text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
the acting , costumes , music , cinematography and sound are all 
\emph on
dumbfounding
\emph default
 given the production 's austere locales .
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example of a successful PWWS attack on a vanilla Bert model, fine tuned
 over the IMDB dataset.
 The original label is 'Positive sentiment'.
 The substituted word, marked in italic font, changed the classification
 to 'Negative sentiment'.
 VUB and VIB classifiers are far less susceptible to these perturbations
 as shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:pwws_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="1">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="8cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Original text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\emph on
great
\emph default
 historical movie, will not allow a viewer to leave once you begin to watch.
 View is presented differently than displayed by most school books on this
 
\emph on
subject
\emph default
.
 My only fault for this movie is it was photographed in black and white;
 wished it had been in color ...
 wow !
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Perturbed text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\emph on
gnreat
\emph default
 historical movie, will not allow a viewer to leave once you begin to watch.
 View is presented differently than displayed by most school books on this
 
\emph on
sSbject
\emph default
.
 My only fault for this movie is it was photographed in black and white;
 wished it had been in color ...
 wow !
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example of a successful Deep Word Bug attack on a vanilla Bert model, fine
 tuned over the IMDB dataset.
 The original label is 'Positive sentiment'.
 Perturbations, marked in italic font, change the classification to 'Negative
 sentiment'.
 VUB and VIB classifiers are far less susceptible to these perturbations,
 as shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../media/info_plane_imdb.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated information plane metrics per epoch for VUB trained on IMDB with
 
\begin_inset Formula $\beta=0.001$
\end_inset

.
 
\begin_inset Formula $I(X;Z)$
\end_inset

 is approximated by 
\begin_inset Formula $H(R)-H(Z|X)$
\end_inset

 and 
\begin_inset Formula $\frac{1}{CE(Y;\hat{Y})}$
\end_inset

 is used as an analog for 
\begin_inset Formula $I(Z;Y)$
\end_inset

.
 The epochs have been grouped and color-coded in intervals of 30 epochs
 in the order: Orange (0-30), gray (30-60), yellow (60-90), green (90-120)
 and red (120-150).
 We notice recurring patterns of distortion reduction followed by rate increase,
 resembling the ERM and representation compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "estimated_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In addition to the above evaluation metrics, we also measured approximated
 rate and distortion throughout training, and plotted them on the information
 plane as shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

 in Appendix
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:AppendixC}
\end_layout

\end_inset

.
 Examining the resulting curve, we notice recurring patterns of distortion
 reduction followed by rate increase, resembling the ERM and representation
 compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 While previous research has documented the occurrence of error minimization
 and representation compression phases 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ShwartzZiv2017}
\end_layout

\end_inset

, our work revealed that these phases can occur in cycles throughout training.
 This finding is particularly noteworthy, because previous studies observed
 this phenomenon in simple toy problems, whereas our research demonstrated
 it in complex tasks of high dimensionality, with unknown distributions.
 This suggests that this information plane behavior is not limited to simplified
 scenarios, but is a possible characteristic of the learning process in
 more challenging tasks as well.
\end_layout

\end_body
\end_document
