@inproceedings{Alemi2017,
  author={Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
  year={2017},
  title={Deep Variational Information Bottleneck},
  booktitle="Proceedings of the International Conference on Learning Representations {(ICLR)}", 
  address={Google Research},
}

@inproceedings{Tishby1999,
  author={Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year={1999},
  title={The Information Bottleneck Method},
  booktitle={The 37th annual Allerton Conference on Communication, Control, and Computing.}, 
  address={Hebrew University, Jerusalem 91904, Israel},
}

@inproceedings{Kingma2014,
  author={Kingma, Diederik P. and Welling, Max},
  year={2014},
  title={Auto-Encoding Variational Bayes},
  booktitle="2nd International Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings",
  eprint={http://arxiv.org/abs/1312.6114v10},
}

@article{Kingma2019,
  author={Kingma, Diederik P. and Welling, Max},
  year={2019},
  title={An Introduction to Variational Autoencoders},
  journal={Foundations and Trends in Machine Learning},
  volume={12},
  number={4},
  pages={307-392},
  URL={http://dblp.uni-trier.de/db/journals/ftml/ftml12.html#KingmaW19},
}

@inproceedings{Higgins2017,
  author={Higgins, Irina and Matthey, Loïc and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year={2017},
  title={beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  booktitle={ICLR (Poster)},
}

@article{Blahut1972,
  author={Blahut, R. E.},
  title={Computation of channel capacity and rate distortion function},
  volume={IT-18},
  pages={460--473},
  year={1972},
  doi={https://ieeexplore.ieee.org/document/1054855},
  publisher={IEEE},
  URL={https://ieeexplore.ieee.org/document/1054855},
  journal={IEEE Transactions on Information Theory},
}

@inproceedings{Carlini2017,
  author={Carlini, Nicholas and Wagner, David A.},
  booktitle={IEEE Symposium on Security and Privacy},
  pages={39-57},
  publisher={IEEE Computer Society},
  title={Towards Evaluating the Robustness of Neural Networks.},
  url={http://dblp.uni-trier.de/db/conf/sp/sp2017.html#Carlini017},
  year={2017}
}

@inproceedings{Goodfellow2015,
  author={Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  booktitle={ICLR (Poster)},
  title={Explaining and Harnessing Adversarial Examples.},
  url={http://dblp.uni-trier.de/db/conf/iclr/iclr2015.html#GoodfellowSS14},
  year={2015}
}

@misc{Tishby2015,
  title={Deep Learning and the Information Bottleneck Principle}, 
  author={Tishby, Naftali and Zaslavsky, Noga},
  year={2015},
  eprint={1503.02406},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}

@article{Painsky2017,
  author={Painsky, Amichai and Tishby, Naftali},
  title={Gaussian Lower Bound for the Information Bottleneck Limit},
  volume={18},
  pages={213:1-213:29},
  year={2017},
  URL={http://dblp.uni-trier.de/db/journals/jmlr/jmlr18.html#PainskyT17},
  journal={J. Mach. Learn. Res.},
}

@article{Wieczorek2019,
  author={Wieczorek, Aleksander and Roth, Volker},
  year={2019},
  title={On the Difference Between the Information Bottleneck and the Deep Information Bottleneck.},
  journal={CoRR},
  volume={abs/1912.13480},
  URL={http://dblp.uni-trier.de/db/journals/corr/corr1912.html#abs-1912-13480},
  eprint={http://arxiv.org/abs/1912.13480},
}

@inproceedings{Chechik2003,
  author={Chechik, Gal and others},
  year={2003},
  title={Gaussian information bottleneck},
  booktitle={Advances in Neural Information Processing Systems},
  URL={https://proceedings.neurips.cc/paper/2003/hash/7e05d6f828574fbc975a896b25bb011e-Abstract.html},
}

@inproceedings{Alemi2018,
  author={Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
  year={2018},
  title={Fixing a Broken ELBO.},
  booktitle={Proceedings of Machine Learning Research},
  pages={159-168},
  volume={80},
  address={PMLR},
  URL={http://dblp.uni-trier.de/db/conf/icml/icml2018.html#AlemiPFDS018},
  eprint={http://proceedings.mlr.press/v80/alemi18a.html},
}

@article{Fischer2020,
  author={Fischer, Ian},
  year={2020},
  title={The Conditional Entropy Bottleneck.},
  journal={Entropy},
  volume={22},
  number={9},
  pages={999},
  URL={http://dblp.uni-trier.de/db/journals/entropy/entropy22.html#Fischer20},
  eprint={https://doi.org/10.3390/e22090999},
}

@article{Geiger2020,
  author={Geiger, Bernhard and Fischer, Ian},
  year={2020},
  title={A Comparison of Variational Bounds for the Information Bottleneck Functional},
  journal={Entropy},
  URL={https://www.mdpi.com/1099-4300/22/11/1229},
}

@article{Achille2018,
  title={Information dropout: Learning optimal representations through noisy computation},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2897--2905},
  year={2018},
  publisher={IEEE}
}

@article{Srivastava2014,
  author={Srivastava, Nitish and Hinton, Geoffrey E and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year={2014},
  title={Dropout: a simple way to prevent neural networks from overfitting},
  journal={Journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  URL={http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf},
}

@inproceedings{Wang2013,
  author={Wang, Sida I. and Manning, Christopher D.},
  year={2013},
  title={Fast dropout training.},
  booktitle={Proceedings of the 30th International Conference on Machine Learning},
  pages={118-126},
  address={JMLR.org},
  URL={http://dblp.uni-trier.de/db/conf/icml/icml2013.html#WangM13},
  eprint={http://proceedings.mlr.press/v28/wang13a.html},
}

@inproceedings{Pereyra2017,
  author={Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, Lukasz and Hinton, Geoffrey E.},
  year={2017},
  title={Regularizing Neural Networks by Penalizing Confident Output Distributions.},
  booktitle={Proceedings of the International Conference on Learning Representations},
  address={OpenReview.net},
  URL={http://dblp.uni-trier.de/db/conf/iclr/iclr2017w.html#PereyraTCKH17},
  eprint={https://openreview.net/forum?id=HyhbYrGYe},
}

@article{Amjad2020,
  author={Amjad, Rana Ali and Geiger, Bernhard C.},
  year={2020},
  title={Learning Representations for Neural Network-Based Classification Using the Information Bottleneck Principle.},
  journal={IEEE Trans. Pattern Anal. Mach. Intell.},
  volume={42},
  number={9},
  pages={2225-2239},
  URL={http://dblp.uni-trier.de/db/journals/pami/pami42.html#AmjadG20},
  eprint={https://www.wikidata.org/entity/Q92876766},
}

@inproceedings{Szegedy2016,
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  year={2016},
  title={Rethinking the inception architecture for computer vision},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818-2826},
}

@misc{ShwartzZiv2017,
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  year={2017},
  title={Opening the Black Box of Deep Neural Networks via Information},
  URL={http://arxiv.org/abs/1703.00810},
  eprint={arXiv:1703.00810},
  note={19 pages, 8 figures},
  archivePrefix={arXiv},
}

@inproceedings{Chen2018,
  author={Chen, Tian Qi and Li, Xuechen and Grosse, Roger B. and Duvenaud, David},
  year={2018},
  title={Isolating Sources of Disentanglement in Variational Autoencoders.},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={2615-2625},
  URL={http://dblp.uni-trier.de/db/conf/nips/nips2018.html#ChenLGD18},
  eprint={http://papers.nips.cc/paper/7527-isolating-sources-of-disentanglement-in-variational-autoencoders},
}

@article{Shamir2010,
  author={Shamir, Ohad and Sabato, Sivan and Tishby, Naftali},
  year={2010},
  title={Learning and generalization with the information bottleneck.},
  journal={Theor. Comput. Sci.},
  volume={411},
  number={29-30},
  pages={2696-2711},
  URL={http://dblp.uni-trier.de/db/journals/tcs/tcs411.html#ShamirST10},
  eprint={http://dx.doi.org/10.1016/j.tcs.2010.04.006},
}

@inproceedings{Garg2020,
  author={Garg, Siddhant and Ramakrishnan, Goutham},
  year={2020},
  title={BAE: BERT-based Adversarial Examples for Text Classification.},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing},
  pages={6174-6181},
  address={Association for Computational Linguistics},
  URL={http://dblp.uni-trier.de/db/conf/emnlp/emnlp2020-1.html#GargR20},
  eprint={https://www.aclweb.org/anthology/2020.emnlp-main.498/},
}

@inproceedings{Devlin2019,
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year={2019},
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  address={Minneapolis, Minnesota},
  URL={https://www.aclweb.org/anthology/N19-1423},
  eprint={10.18653/v1/N19-1423},
}

@inproceedings{Zhang2017,
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year={2017},
  title={Understanding deep learning requires rethinking generalization},
  booktitle={Proceedings of the International Conference on Learning Representations (ICLR)},
}

@inproceedings{Morris2020,
  author={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  year={2020},
  title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={119--126},
}

@misc{Kaiwen2018,
  author={Kaiwen},
  title={pytorch-cw2},
  year={2018},
  url={https://github.com/kkew3/pytorch-cw2},
  note={GitHub repository}
}

@article{Ying2019,
doi={10.1088/1742-6596/1168/2/022022},
url={https://dx.doi.org/10.1088/1742-6596/1168/2/022022},
year={2019},
month={feb},
publisher={IOP Publishing},
volume={1168},
number={2},
pages={022022},
author={Xue Ying},
title={An Overview of Overfitting and its Solutions},
journal={Journal of Physics: Conference Series},
abstract={Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) “early-stopping” strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) “network-reduction” strategy is used to exclude the noises in training set; 3) “data-expansion” strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) “regularization” strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.}
}

@inproceedings{Guo2017,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@phdthesis{Slonim2002,
  title={The information bottleneck: Theory and applications},
  author={Slonim, Noam},
  year={2002},
  school={Hebrew University of Jerusalem Jerusalem, Israel}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{maas2011imdb,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

@inproceedings{gao2018deepwordbug,
  title={Black-box generation of adversarial text sequences to evade deep learning classifiers},
  author={Gao, Ji and Lanchantin, Jack and Soffa, Mary Lou and Qi, Yanjun},
  booktitle={2018 IEEE Security and Privacy Workshops (SPW)},
  pages={50--56},
  year={2018},
  organization={IEEE}
}

@inproceedings{ren2019pwws,
  title={Generating natural language adversarial examples through probability weighted word saliency},
  author={Ren, Shuhuai and Deng, Yihe and He, Kun and Che, Wanxiang},
  booktitle={Proceedings of the 57th annual meeting of the association for computational linguistics},
  pages={1085--1097},
  year={2019}
}

@article{Deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}

@inproceedings{Ioffe2015BatchNA,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Sergey Ioffe and Christian Szegedy},
  booktitle={International Conference on Machine Learning},
  year={2015},
  url={https://api.semanticscholar.org/CorpusID:5808102}
}

@inproceedings{szegedy2015going,
  added-at = {2022-07-15T12:12:27.000+0200},
  author = {"Szegedy, Christian" and "Liu, Wei" and "Jia, Yangqing" and "Sermanet, Pierre" and "Reed, Scott" and "Anguelov, Dragomir" and "Erhan, Dumitru" and "Vanhoucke, Vincent" and "Rabinovich, Andrew"},
  biburl = {https://www.bibsonomy.org/bibtex/2d0207c3f3970a0e30bebfe158447c0d0/ariane.mueller},
  interhash = {565b6da73b5f9e935fba89d1c27a524b},
  intrahash = {d0207c3f3970a0e30bebfe158447c0d0},
  keywords = {},
  timestamp = {2022-07-15T12:12:27.000+0200},
  title = {Going deeper with convolutions},
  year = 2015
}

@article{Krizhevsky2009learning,
  added-at = {2021-01-21T03:01:11.000+0100},
  author = {Krizhevsky, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/2fe5248afe57647d9c85c50a98a12145c/s364315},
  interhash = {cc2d42f2b7ef6a4e76e47d1a50c8cd86},
  intrahash = {fe5248afe57647d9c85c50a98a12145c},
  keywords = {},
  pages = {32--33},
  timestamp = {2021-01-21T03:01:11.000+0100},
  title = {Learning Multiple Layers of Features from Tiny Images},
  url = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
  year = 2009
}

@misc{Xiao2017fashionmnist,
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images
of 70,000 fashion products from 10 categories, with 7,000 images per category.
The training set has 60,000 images and the test set has 10,000 images.
Fashion-MNIST is intended to serve as a direct drop-in replacement for the
original MNIST dataset for benchmarking machine learning algorithms, as it
shares the same image size, data format and the structure of training and
testing splits. The dataset is freely available at
https://github.com/zalandoresearch/fashion-mnist},
  added-at = {2021-10-12T06:50:19.000+0200},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  biburl = {https://www.bibsonomy.org/bibtex/2de51af2f6c7d8b0f4cd84a428bb17967/andolab},
  description = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  interhash = {0c81f9a6170118f14703b6796101ce40},
  intrahash = {de51af2f6c7d8b0f4cd84a428bb17967},
  keywords = {Fashion-MNIST Image_Classification_Benchmark},
  note = {cite arxiv:1708.07747Comment: Dataset is freely available at  https://github.com/zalandoresearch/fashion-mnist Benchmark is available at  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/},
  timestamp = {2023-01-31T20:34:07.000+0100},
  title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning
  Algorithms},
  url = {http://arxiv.org/abs/1708.07747},
  year = 2017
}

@book{Cover1999elements,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}

@article{Levenshtein1966binary,
  added-at = {2014-12-08T12:04:30.000+0100},
  author = {Levenshtein, Vladimir I},
  biburl = {https://www.bibsonomy.org/bibtex/2c1d45b158a34ca29d280263a14dfd981/sdo},
  interhash = {55f7ad93fcb9ae3ed999afaa6e24937d},
  intrahash = {c1d45b158a34ca29d280263a14dfd981},
  journal = {Soviet Physics Doklady},
  keywords = {},
  month = {February},
  pages = 707,
  timestamp = {2014-12-08T12:04:30.000+0100},
  title = {Binary Codes Capable of Correcting Deletions, Insertions and Reversals},
  volume = 10,
  year = 1966
}

@inproceedings{Li2016,
    title = "Visualizing and Understanding Neural Models in {NLP}",
    author = "Li, Jiwei  and
      Chen, Xinlei  and
      Hovy, Eduard  and
      Jurafsky, Dan",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1082",
    doi = "10.18653/v1/N16-1082",
    pages = "681--691",
}

@article{Miller1990,
  added-at = {2010-09-03T15:19:16.000+0200},
  author = {Miller, G. A. and Beckwith, R. and Fellbaum, C. and Gross, D. and Miller, K. J.},
  biburl = {https://www.bibsonomy.org/bibtex/260751ad9587adaa781251b17d00ca2c3/tgunkel},
  interhash = {d61e6e1fa4584d5eb7724c52454d3209},
  intrahash = {60751ad9587adaa781251b17d00ca2c3},
  journal = {International Journal of Lexicography},
  keywords = {diplom wordnet},
  number = 4,
  owner = {BlEv},
  pages = {235-244},
  timestamp = {2010-09-03T15:19:16.000+0200},
  title = {{Introduction to WordNet: an on-line lexical database}},
  url = {http://wordnetcode.princeton.edu/5papers.pdf},
  volume = 3,
  year = 1990
}

@inproceedings{Zhang2015,
  title={Character-level Convolutional Networks for Text Classification},
  author={Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
  booktitle={NIPS},
  year={2015}
}

@book{Cover2006,
  added-at = {2009-04-20T21:27:16.000+0200},
  at = {2008-03-31 06:17:47},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  biburl = {https://www.bibsonomy.org/bibtex/22e9bfa879286689a14feb55b69d326c1/ywhuang},
  howpublished = {Hardcover},
  id = {1877660},
  interhash = {87ae368776946bf7a71ee476e81a2191},
  intrahash = {2e9bfa879286689a14feb55b69d326c1},
  isbn = {0471241954},
  keywords = {information-theory book},
  month = {July},
  priority = {0},
  publisher = {Wiley-Interscience},
  timestamp = {2009-04-20T21:27:16.000+0200},
  title = {Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)},
  year = 2006
}

@article{Saxe2018,
  added-at = {2021-01-23T01:02:23.000+0100},
  author = {Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu},
  biburl = {https://www.bibsonomy.org/bibtex/23a0bc76185880e07c14dfe37b1ae7539/cpankow},
  description = {On the information bottleneck theory of deep learning | Request PDF},
  interhash = {66ded0e0d550e9a02b6496cbc14029af},
  intrahash = {3a0bc76185880e07c14dfe37b1ae7539},
  keywords = {generalization informationtheory machinelearning neuralnetwork},
  timestamp = {2021-01-23T01:02:23.000+0100},
  title = {On the information bottleneck theory of deep learning},
  year = 2018
}

@Article{Kolchinsky2017,
AUTHOR = {Kolchinsky, Artemy and Tracey, Brendan D.},
TITLE = {Estimating Mixture Entropy with Pairwise Distances},
JOURNAL = {Entropy},
VOLUME = {19},
YEAR = {2017},
NUMBER = {7},
ARTICLE-NUMBER = {361},
URL = {https://www.mdpi.com/1099-4300/19/7/361},
ISSN = {1099-4300},
ABSTRACT = {Mixture distributions arise in many parametric and non-parametric settings—for example, in Gaussian mixture models and in non-parametric estimation. It is often necessary to compute the entropy of a mixture, but, in most cases, this quantity has no closed-form expression, making some form of approximation necessary. We propose a family of estimators based on a pairwise distance function between mixture components, and show that this estimator class has many attractive properties. For many distributions of interest, the proposed estimators are efficient to compute, differentiable in the mixture parameters, and become exact when the mixture components are clustered. We prove this family includes lower and upper bounds on the mixture entropy. The Chernoff α -divergence gives a lower bound when chosen as the distance function, with the Bhattacharyaa distance providing the tightest lower bound for components that are symmetric and members of a location family. The Kullback–Leibler divergence gives an upper bound when used as the distance function. We provide closed-form expressions of these bounds for mixtures of Gaussians, and discuss their applications to the estimation of mutual information. We then demonstrate that our bounds are significantly tighter than well-known existing bounds using numeric simulations. This estimator class is very useful in optimization problems involving maximization/minimization of entropy and mutual information, such as MaxEnt and rate distortion problems.},
DOI = {10.3390/e19070361}
}

@article{Kraskov2004,
  title = {Estimating mutual information},
  author = {Kraskov, Alexander and St\"ogbauer, Harald and Grassberger, Peter},
  journal = {Phys. Rev. E},
  volume = {69},
  issue = {6},
  pages = {066138},
  numpages = {16},
  year = {2004},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.69.066138},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138}
}


@InProceedings{Cheng2020,
  title =    {{CLUB}: A Contrastive Log-ratio Upper Bound of Mutual Information},
  author =       {Cheng, Pengyu and Hao, Weituo and Dai, Shuyang and Liu, Jiachang and Gan, Zhe and Carin, Lawrence},
  booktitle =    {Proceedings of the 37th International Conference on Machine Learning},
  pages =    {1779--1788},
  year =   {2020},
  editor =   {III, Hal Daumé and Singh, Aarti},
  volume =   {119},
  series =   {Proceedings of Machine Learning Research},
  month =    {13--18 Jul},
  publisher =    {PMLR},
  pdf =    {http://proceedings.mlr.press/v119/cheng20b/cheng20b.pdf},
  url =    {https://proceedings.mlr.press/v119/cheng20b.html},
  abstract =   {Mutual information (MI) minimization has gained considerable interests in various machine learning tasks. However, estimating and minimizing MI in high-dimensional spaces remains a challenging problem, especially when only samples, rather than distribution forms, are accessible. Previous works mainly focus on MI lower bound approximation, which is not applicable to MI minimization problems. In this paper, we propose a novel Contrastive Log-ratio Upper Bound (CLUB) of mutual information. We provide a theoretical analysis of the properties of CLUB and its variational approximation. Based on this upper bound, we introduce a MI minimization training scheme and further accelerate it with a negative sampling strategy. Simulation studies on Gaussian distributions show the reliable estimation ability of CLUB. Real-world MI minimization experiments, including domain adaptation and information bottleneck, demonstrate the effectiveness of the proposed method. The code is at https://github.com/Linear95/CLUB.}
}

@article{Shannon1948,
  added-at = {2021-09-19T18:40:37.000+0200},
  author = {Shannon, Claude Elwood},
  biburl = {https://www.bibsonomy.org/bibtex/29f88587b33c82f692b61d129eb2f2517/steschum},
  interhash = {754130207906fcec16a53d330eeff348},
  intrahash = {9f88587b33c82f692b61d129eb2f2517},
  journal = {The Bell System Technical Journal},
  keywords = {imported},
  pages = {379--423},
  timestamp = {2021-09-19T18:41:56.000+0200},
  title = {A Mathematical Theory of Communication},
  url = {http://plan9.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
  urldate = {2003-04-22},
  volume = 27,
  year = 1948
}

@inproceedings{Shannon1959,
  title={Coding Theorems for a Discrete Source With a Fidelity Criterion},
  author={Claude E. Shannon},
  year={1959},
  booktitle={IRE National Convention},
}

@ARTICLE{Arimoto1972,
  author={Arimoto, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={An algorithm for computing the capacity of arbitrary discrete memoryless channels}, 
  year={1972},
  volume={18},
  number={1},
  pages={14-20},
  keywords={},
  doi={10.1109/TIT.1972.1054753}}

@article{Zaidi2020,
  title={On the Information Bottleneck Problems: Models, Connections, Applications and Information Theoretic Views},
  author={Abdellatif Zaidi and I{\~n}aki Estella Aguerri and Shlomo Shamai},
  journal={Entropy},
  year={2020},
  volume={22},
  url={https://api.semanticscholar.org/CorpusID:211010616}
}

@ARTICLE{Goldfeld2020,
  author={Goldfeld, Ziv and Polyanskiy, Yury},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={The Information Bottleneck Problem and its Applications in Machine Learning}, 
  year={2020},
  volume={1},
  number={1},
  pages={19-38},
  keywords={Mutual information;Deep learning;Training;IP networks;Noise measurement;Deep learning;information bottleneck;machine learning;mutual information;neural networks},
  doi={10.1109/JSAIT.2020.2991561}}

@inproceedings{Barber2003,
  title={The IM algorithm: a variational approach to Information Maximization},
  author={David Barber and Felix V. Agakov},
  booktitle={Neural Information Processing Systems},
  year={2003},
  url={https://api.semanticscholar.org/CorpusID:14633080}
}

@inproceedings{Belghazi2018,
  title={Mutual Information Neural Estimation},
  author={Mohamed Ishmael Belghazi and Aristide Baratin and Sai Rajeswar and Sherjil Ozair and Yoshua Bengio and R. Devon Hjelm and Aaron C. Courville},
  booktitle={International Conference on Machine Learning},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:44220142}
}

@inproceedings{MagdonIsmail1998,
  title={Neural Networks for Density Estimation},
  author={Malik Magdon-Ismail and Amir F. Atiya},
  booktitle={Neural Information Processing Systems},
  year={1998},
  url={https://api.semanticscholar.org/CorpusID:9459664}
}